# AI Usage Log: Mental Health LLM Safety Benchmark (CSY3055 PJ1)

**Project**: Reliable Clinical Benchmark - Uni-setup  
**Student**: Ryan Mutiga Gichuru  
**Module**: CSY3055 Natural Language Processing  
**Date Range**: November 2025 - January 2026

---

## Executive Summary

This document provides a comprehensive log of AI tool usage throughout the development of the mental health LLM safety benchmark project. AI assistance was used **exclusively for code implementation, debugging, and engineering tasks**, whilst all research design, architecture decisions, literature review, and theoretical framework development were led by the student.

**AI Tools Used**: Cursor AI (primary), ChatGPT (occasional clarification)  
**Scope**: Code generation, unit test creation, error debugging, metric implementation  
**Excluded**: Report writing, analysis, literature review, research design

**Critical Note on Code Acceptance**: All AI-generated code updates were **manually reviewed and accepted** by the student before integration. The student is fully liable for all code in the repository. Every line of AI-generated code was:
- Reviewed for correctness
- Validated against specifications
- Tested for edge cases
- Approved before commit

This ensures the student maintains full epistemic agency and responsibility for the implementation.

**GitHub Commit Evidence**: The repository shows **178 commits** from November 13, 2024 to December 17, 2025, demonstrating consistent daily work and incremental progress tracking. Commits span different times of day, confirming work from multiple devices as stated by the student. This commit history provides transparent evidence of the student's active engagement and careful validation process throughout the project.

---

## 1. Unit Tests (100% AI-Generated)

### 1.1 Test Files Created
All unit test files were generated by AI based on specifications provided by the student. The student defined:
- What functionality needed testing
- Expected behaviours and edge cases
- Test data requirements

**AI-Generated Test Files**:
- `src/tests/models/test_piaget_local.py` - Smoke tests for Piaget-8B local runner
- `src/tests/models/test_psyllm_local.py` - PsyLLM model integration tests
- `src/tests/models/test_psych_qwen_local.py` - Psych_Qwen_32B runner tests
- `src/tests/models/test_psyche_r1_local.py` - Psyche-R1 model tests
- `src/tests/models/test_psyllm_gml_local.py` - PsyLLM GML variant tests
- `src/tests/lmstudio/test_lmstudio_capture_*.py` - LM Studio integration tests for multiple models
- `tests/unit/metrics/test_faithfulness_metrics.py` - Faithfulness metric unit tests
- `tests/unit/metrics/test_sycophancy_metrics.py` - Sycophancy metric unit tests
- `tests/unit/metrics/test_drift_metrics.py` - Longitudinal drift metric tests
- `tests/unit/study_a/test_study_a_metrics_calculation.py` - Study A calculation tests
- `tests/unit/runners/test_*.py` - Model runner integration tests
- `tests/integration/test_*.py` - End-to-end pipeline tests

**Student Contribution**: 
- Defined test requirements and expected behaviours
- Reviewed and validated test logic
- Identified missing test cases
- Ensured tests align with evaluation protocol

**AI Contribution**:
- Generated complete test file structure
- Implemented pytest fixtures and test cases
- Created mock data and test utilities
- Wrote assertions and edge case handling

---

## 2. Complex Code Implementation (AI-Generated with Student Guidance)

### 2.1 Metric Calculation Modules
The metric calculation files represent the most complex implementation work, translating mathematical definitions from `Metrics and Evaluation.tex` into working Python code.

**Files**:
- `src/reliable_clinical_benchmark/metrics/faithfulness.py` - Faithfulness Gap ($\Delta$), Step-F1, Silent Bias Rate
- `src/reliable_clinical_benchmark/metrics/sycophancy.py` - Sycophancy Probability ($P_{\text{Syc}}$), Flip Rate, Evidence Hallucination
- `src/reliable_clinical_benchmark/metrics/drift.py` - Entity Recall Decay, Knowledge Conflict, Truth Decay Rate
- `src/reliable_clinical_benchmark/metrics/extraction.py` - Output parsing, diagnosis extraction, complexity metrics
- `src/reliable_clinical_benchmark/metrics/utils.py` - Shared utilities (NLI, NER, text processing)

**Reference Documentation Used**:
The student created comprehensive documentation files in `docs/` that served as references for mathematical conversions:
- `docs/metrics/METRIC_CALCULATION_PIPELINE.md` - Detailed pipeline from text to metrics, with LaTeX formula alignment
- `docs/metrics/QUICK_REFERENCE.md` - Quick reference for all metrics with implementation locations
- `docs/data/STUDY_A_GOLD_LABELS_MAPPING.md` - Gold label extraction methodology
- `docs/evaluation/EVALUATION_PROTOCOL.md` - Complete evaluation procedures

These documentation files were created by the student to ensure mathematical correctness and served as the primary reference during AI-assisted implementation. Some implementations leveraged existing libraries (e.g., Dice coefficient for token overlap, scispaCy for NER, NLI models for contradiction detection) where appropriate, but all metric formulas were implemented from first principles to match the LaTeX specification.

**Student Contribution**:
- Provided mathematical definitions and formulas from LaTeX document
- Created reference documentation files for mathematical conversions
- Explained clinical interpretation requirements
- Specified edge cases and failure modes
- Validated algorithm correctness against evaluation protocol
- Identified need for gold label closed-set matching (see Section 5)
- Reviewed all library integrations to ensure they match specifications

**AI Contribution**:
- Implemented mathematical formulas as Python functions based on student documentation
- Created data structures for metric computation
- Handled edge cases (empty outputs, malformed responses, etc.)
- Integrated external libraries (scispaCy, NLI models, sentence transformers) as specified by student
- Optimised performance for large-scale evaluation

**Example**: The Faithfulness Gap calculation required:
1. Running models in both CoT and direct modes
2. Extracting diagnoses from structured outputs
3. Comparing accuracies
4. Handling cases where models refuse or produce invalid outputs

The AI implemented the complete pipeline, whilst the student ensured it matched the protocol specification.

---

## 3. Documentation and Logic (Student-Led, AI-Assisted)

### 3.1 Research Design and Architecture
**Student Contribution**: 100% of research design, literature review, and theoretical framework.

- **Literature Review**: Student identified, read, and synthesised all sources. Student read papers, found references, and extracted relevant methodologies (Lanham et al. for faithfulness, Wei et al. for sycophancy, Turpin et al. for silent bias, etc.)
- **Environment Setup**: Student configured development environments, conda environments, and dependencies. Student ensured reproducibility across different systems.
- **Evaluation Protocol**: Designed by student based on clinical safety requirements
- **Metric Selection**: Student selected metrics from literature (Lanham et al., Wei et al., Turpin et al.)
- **Model Selection**: Student chose models based on research questions (scale, specialisation, architecture)
- **Architecture Design**: Student designed modular system architecture for community extension

**AI Assistance**: 
- Formatting LaTeX documents
- Clarifying technical terminology
- Suggesting code organisation patterns (not research decisions)

### 3.2 Translation from LaTeX to Code
The `Metrics and Evaluation.tex` document (1,805 lines) contains complete mathematical specifications. The student guided the translation process:

**Student Process**:
1. Identified which sections needed implementation
2. Explained mathematical notation to AI
3. Specified clinical interpretation requirements
4. Validated that implementations matched specifications
5. Identified gaps (e.g., gold label extraction - see Section 5)

**AI Process**:
1. Read LaTeX formulas and pseudocode
2. Generated Python implementations
3. Asked clarifying questions about edge cases
4. Refactored based on student feedback

**Key Example**: The Faithfulness Gap formula:

$$\Delta_{\text{Reasoning}} = \text{Acc}_{\text{CoT}} - \text{Acc}_{\text{Early}}$$

Student explained:
- $\text{Acc}_{\text{CoT}}$ requires structured REASONING: and DIAGNOSIS: sections
- $\text{Acc}_{\text{Early}}$ uses [SKIP] in reasoning section
- Both need to extract same diagnosis format for fair comparison

AI implemented the prompt templates, extraction logic, and comparison functions.

---

## 4. Infrastructure Setup (Joint Effort)

### 4.1 LM Studio Configuration
**Joint Effort**: Student and AI worked together to:
- Configure LM Studio for local model inference
- Set up API endpoints for different models
- Handle model-specific prompt formatting
- Debug connection issues and timeout errors

**Student Contribution**:
- Identified need for local inference (no API costs)
- Selected LM Studio as inference server
- Configured model-specific settings (temperature, top_p, max_tokens)
- Validated that generations matched expected formats

**AI Contribution**:
- Generated API client code for LM Studio
- Implemented retry logic and error handling
- Created model-specific adapters (Piaget-8B, PsyLLM, etc.)
- Fixed connection and serialisation bugs

### 4.2 Study Generation Pipeline
**Joint Effort**: Ensuring study generations run correctly and are saved properly.

**Student Contribution**:
- Designed study execution workflow
- Specified output format (JSONL with metadata)
- Identified requirements for run_id, timestamps, error tracking
- Validated that saved outputs match evaluation protocol

**AI Contribution**:
- Implemented generation orchestration scripts
- Created JSONL writing utilities with proper error handling
- Added progress tracking and logging
- Implemented checkpoint/resume functionality for long runs

**Files**:
- `src/reliable_clinical_benchmark/studies/study_a.py`
- `src/reliable_clinical_benchmark/studies/study_b.py`
- `src/reliable_clinical_benchmark/studies/study_c.py`
- `scripts/study_*/generate_*.py`

### 4.3 Error Tracking and Debugging
**Joint Effort**: Systematic debugging of runtime errors.

**Student Contribution**:
- Identified error patterns (timeouts, malformed outputs, connection failures)
- Prioritised which errors to fix first
- Validated that fixes didn't break existing functionality

**AI Contribution**:
- Analysed error traces and stack traces
- Suggested fixes for common issues
- Implemented error recovery mechanisms
- Added comprehensive error logging

**Common Issues Resolved**:
- Model output parsing failures (missing REASONING: tags)
- LM Studio connection timeouts
- JSON serialisation errors with special characters
- Memory issues with large model outputs

---

## 5. Metric Calculation Files (Mutual Effort)

### 5.1 Study A: Gold Label Extraction (Student-Identified Gap)

**Critical Discovery**: During implementation, the student identified that the original design did not account for:
1. Gold standard diagnosis labels for accuracy calculation
2. Closed-set matching (models may output free-text diagnoses)
3. Additional metrics that could be helpful (complexity, refusal rates)

**Student Action**:
- Identified the gap after reviewing initial AI-generated extraction code
- Specified requirements for gold label matching
- Designed closed-set diagnosis taxonomy
- Requested implementation of additional diagnostic metrics

**AI Implementation**:
- Created `scripts/study_a/metrics/extract_predictions.py` with gold label matching
- Implemented `extract_diagnosis_closed_set()` function
- Added `compute_complexity_metrics()` for output analysis
- Created validation scripts to ensure extraction correctness

**Files Created**:
- `scripts/study_a/metrics/extract_predictions.py` - Main extraction script
- `scripts/study_a/metrics/validate_extracted_data.py` - Validation and quality checks
- `scripts/study_a/metrics/analyze_extraction.py` - Diagnostic analysis
- `scripts/study_a/metrics/calculate_metrics.py` - Metric computation from extracted data
- `processed/study_a_extracted/` - Processed outputs with gold labels

**Impact**: This discovery significantly improved the evaluation framework's robustness and enabled proper accuracy calculations for Study A.

### 5.2 Metric Calculation Implementation

**Student Contribution**:
- Specified metric formulas from LaTeX document
- Explained clinical interpretation (e.g., what constitutes "agreement" in sycophancy)
- Validated results against expected baselines
- Identified edge cases requiring special handling

**AI Contribution**:
- Implemented metric calculation functions
- Created efficient batch processing for large datasets
- Added statistical validation (confidence intervals, bootstrap resampling)
- Generated visualisation code for metric reporting

**Key Metrics Implemented**:
- **Faithfulness Gap** ($\Delta_{\text{Reasoning}}$): $\text{Acc}_{\text{CoT}} - \text{Acc}_{\text{Early}}$ with proper prompt matching
- **Step-F1**: Token overlap with gold reasoning using ROUGE-style matching
- **Sycophancy Probability** ($P_{\text{Syc}}$): Agreement rate shift (Control vs Injected)
- **Flip Rate**: Correct â†’ Incorrect transitions
- **Entity Recall Decay**: Medical entity retention over multi-turn conversations
- **Knowledge Conflict** ($K_{\text{Conflict}}$): NLI-based contradiction detection

---

## 6. Code Comments and References

### 6.1 AI-Generated Code Marking
All code sections that were fully AI-generated are marked with comments. Examples:

```python
# AI-generated: Cursor AI - Model runner implementation
# Based on student specification of ModelRunner interface
class Piaget8BLocalRunner(ModelRunner):
    ...
```

```python
# AI-generated: Cursor AI - Unit test structure
# Student defined test requirements and expected behaviours
def test_piaget_cot_generation():
    ...
```

### 6.2 Student-Guided Code
Code where student provided significant guidance is marked:

```python
# Student-guided: Translation from Metrics and Evaluation.tex Section 6.1
# Student explained mathematical notation and clinical interpretation
def calculate_faithfulness_gap(cot_results, early_results):
    ...
```

---

## 7. Decision-Making Overview

### 7.1 Student Decisions (No AI Input)

1. **Research Questions**: 
   - How does model scale affect safety?
   - Does domain specialisation improve safety?
   - How do reasoning models compare to base models?

2. **Metric Selection**:
   - Chose Faithfulness Gap over other faithfulness measures
   - Selected Sycophancy Probability as primary sycophancy metric
   - Prioritised Entity Recall over other drift measures

3. **Model Selection**:
   - Selected 5 core models (PsyLLM, QwQ-32B, DeepSeek-R1-14B, GPT-OSS-120B, Qwen3-8B)
   - Identified 3 extension models (Psych_Qwen_32B, Piaget-8B, Psyche-R1)
   - Rationale: Cover scale range (8B-120B) and specialisation spectrum

4. **Architecture Design**:
   - Modular ModelRunner interface for community extension
   - Frozen test splits for reproducibility
   - JSONL output format for compatibility

5. **Evaluation Protocol**:
   - 15% failure buffer for robustness
   - Black-box only (no white-box metrics)
   - Clinical safety thresholds

### 7.2 AI-Assisted Decisions (Student Final Authority)

1. **Code Organisation**:
   - AI suggested directory structure
   - Student approved/modified based on project needs

2. **Error Handling Strategy**:
   - AI suggested retry logic and error recovery
   - Student validated against clinical requirements

3. **Performance Optimisation**:
   - AI suggested batching and caching strategies
   - Student approved based on compute constraints

---

## 8. Files and Their AI Usage Status

### 8.1 Fully AI-Generated (with Student Requirements)
- All test files (`src/tests/**/*.py`, `tests/**/*.py`)
- Model runner implementations (`src/reliable_clinical_benchmark/models/*.py`)
- LM Studio integration code (`src/reliable_clinical_benchmark/runners/lmstudio.py`)
- Utility functions (`src/reliable_clinical_benchmark/utils/*.py`)

### 8.2 Student-Guided AI Implementation
- Metric calculation modules (`src/reliable_clinical_benchmark/metrics/*.py`)
- Study execution scripts (`src/reliable_clinical_benchmark/studies/*.py`)
- Extraction and processing scripts (`scripts/study_*/metrics/*.py`)
- Data validation scripts (`scripts/study_a/metrics/validate_extracted_data.py`)

### 8.3 Student-Created (Minimal AI Assistance)
- Evaluation protocol document (`docs/evaluation/EVALUATION_PROTOCOL.md`)
- Environment setup guide (`docs/environment/ENVIRONMENT.md`)
- README and project documentation (`README.md`)
- LaTeX specification (`docs/Guides/Metrics and Evaluation.tex`)

### 8.4 Joint Development
- Configuration files (`config/*.yaml`, `requirements.txt`)
- Generation orchestration (`scripts/study_*/generate_*.py`)
- Error handling and logging infrastructure

---

## 9. Specific AI Prompts and Outcomes

### 9.1 Example: Faithfulness Gap Implementation

**Student Prompt**:
> "I need to implement the Faithfulness Gap metric from the LaTeX document. The formula is $\Delta = \text{Acc}_{\text{CoT}} - \text{Acc}_{\text{Early}}$. I need to run models in two modes: CoT (with REASONING: section) and direct (with [SKIP] in reasoning). Both should extract diagnoses from DIAGNOSIS: section. Then calculate accuracy for each mode and subtract."

**AI Output**:
- Complete implementation with prompt templates
- Diagnosis extraction logic
- Accuracy calculation with error handling
- Integration with model runner interface

**Student Validation**:
- Verified prompt templates match protocol
- Tested on sample outputs
- Confirmed edge case handling (refusals, malformed outputs)

### 9.2 Example: Unit Test Generation

**Student Prompt**:
> "Create unit tests for the faithfulness metrics. Test cases should cover: normal CoT/Early comparison, empty outputs, malformed diagnosis extraction, and edge cases where models refuse to answer."

**AI Output**:
- Complete pytest test suite
- Fixtures for test data
- Mock model responses
- Assertions for all specified cases

**Student Validation**:
- Ran tests and verified they catch real bugs
- Added additional test cases for discovered edge cases
- Ensured tests align with evaluation protocol

---

## 10. Time and Effort Breakdown

### 10.1 Student Time Investment (Based on GitHub Commit History)

**Analysis Period**: November 13, 2024 - December 17, 2025 (35 days)  
**Total Commits**: 178 commits (verified via GitHub repository)  
**Commit Pattern**: Multiple commits per day, across different times, indicating work from multiple devices and consistent daily engagement

**Important Note**: The time investment documented below reflects work completed **up to Study A completion**. Study A is fully implemented and validated. Earlier commits included some Study B and C infrastructure work, but significant additional time investment is expected for:
- Perfecting Study B (Sycophancy) implementation and validation
- Perfecting Study C (Longitudinal Drift) implementation and validation
- Calculating metrics across all three studies
- Statistical analysis and validation
- Writing the 1500-word report
- Preparing for viva/demonstration

**Current Status**: Study A complete; Studies B and C in progress

**Research and Design** (100% student):
- Literature review and paper reading: ~18 hours
- Finding and synthesising references: ~8 hours
- Evaluation protocol design: ~12 hours
- Metric selection and rationale: ~10 hours
- Model selection: ~8 hours
- Architecture design: ~10 hours
- **Total: ~66 hours**

**Implementation Guidance** (Student-led):
- Translating LaTeX to code specifications: ~15 hours
- Code review and validation (all commits manually reviewed): ~25 hours
- Debugging and error identification: ~15 hours
- Gap identification (gold labels, extraction crisis, etc.): ~8 hours
- **Total: ~63 hours**

**Infrastructure Setup** (Joint effort, student-led):
- Environment setup (conda, dependencies, multiple devices): ~8 hours
- LM Studio configuration and validation: ~6 hours
- Study generation validation and testing: ~10 hours
- Error tracking and debugging setup: ~5 hours
- **Total: ~29 hours (student portion)**

**Documentation and Quality Assurance**:
- Creating reference documentation (`docs/` folder): ~12 hours
- Writing evaluation protocols: ~8 hours
- Code comments and AI usage tracking: ~5 hours
- Manual validation and testing: ~15 hours
- **Total: ~40 hours**

**Total Student Time (to Study A completion): ~198 hours** (approximately 5.7 hours per day over 35 days)

**Estimated Remaining Work**:
- Study B completion and validation: ~25-30 hours
- Study C completion and validation: ~25-30 hours
- Metrics calculation across all studies: ~15-20 hours
- Statistical analysis and validation: ~10-15 hours
- Report writing (1500 words): ~20-25 hours
- Viva/demonstration preparation: ~8-10 hours

**Projected Total Time Investment: ~300-330 hours**

**Note**: The commit history demonstrates consistent daily work, with commits spanning different times of day (indicating work from different devices/locations). The high commit frequency (178 commits) reflects the student's practice of committing work incrementally, ensuring all progress is tracked even when working across multiple devices. The actual time investment significantly exceeds typical module expectations (~120 hours for a 60% weighted assessment), reflecting the dissertation-level complexity of this project.

### 10.2 AI-Assisted Time Savings

Without AI assistance, estimated additional time:
- Unit test creation: ~20 hours â†’ **Saved: ~18 hours**
- Metric implementation: ~35 hours â†’ **Saved: ~28 hours**
- Error debugging: ~18 hours â†’ **Saved: ~14 hours**
- Code boilerplate: ~12 hours â†’ **Saved: ~9 hours**

**Total Time Saved: ~69 hours**

**Note**: Time savings enabled the student to focus on research design, validation, and ensuring clinical correctness rather than repetitive coding tasks. The actual time investment of ~198 hours demonstrates significant student engagement, with AI tools accelerating implementation whilst maintaining quality through comprehensive review (evidenced by 178 commits showing incremental progress and validation).

### 10.3 GitHub Commit Analysis

**Commit Statistics** (November 13, 2024 - December 17, 2025):
- **Total Commits**: 178 commits
- **Average Commits per Day**: ~5.1 commits/day
- **Work Pattern**: Consistent daily commits across different times, indicating:
  - Work from multiple devices (as stated by student)
  - Incremental progress tracking
  - Regular validation and testing cycles
  - Active development over 35-day period

**Commit Categories** (based on commit messages, up to Study A completion):
- **Model Implementation**: ~45 commits (runners, tests, configurations)
- **Metric Implementation**: ~35 commits (faithfulness metrics complete; sycophancy and drift in progress)
- **Infrastructure**: ~30 commits (LM Studio, environment setup, caching)
- **Documentation**: ~25 commits (docs, READMEs, protocols)
- **Data Processing**: ~20 commits (gold labels, extraction, validation - Study A complete)
- **Bug Fixes and Refinements**: ~23 commits (error handling, edge cases)

**Study Progress** (as of December 17, 2025):
- **Study A (Faithfulness)**: âœ… Complete - fully implemented, validated, metrics calculated
- **Study B (Sycophancy)**: ðŸ”„ In Progress - infrastructure in place, generation scripts ready, metrics implementation ongoing
- **Study C (Longitudinal Drift)**: ðŸ”„ In Progress - infrastructure in place, generation scripts ready, metrics implementation ongoing

**Evidence of Student Engagement**:
- Multiple commits per day show active, sustained work
- Commit messages demonstrate understanding of changes
- Incremental commits indicate careful validation at each step
- Cross-device commits (different timestamps) confirm student's statement about working from multiple devices

---

## 11. Quality Assurance

### 11.1 Student Validation Process

1. **Code Review**: All AI-generated code reviewed line-by-line
2. **Functional Testing**: All implementations tested against evaluation protocol
3. **Mathematical Verification**: Formulas verified against LaTeX specifications
4. **Clinical Validation**: Outputs validated for clinical interpretability
5. **Edge Case Testing**: Comprehensive testing of failure modes

### 11.2 Discovered Issues and Fixes

**Issue 1**: Initial extraction didn't handle gold label matching
- **Student Identified**: During validation of Study A results
- **Fix**: Implemented closed-set diagnosis taxonomy (Section 5.1)

**Issue 2**: LM Studio connection timeouts not handled gracefully
- **Student Identified**: During long study generation runs
- **Fix**: Added retry logic with exponential backoff

**Issue 3**: Model outputs sometimes missing required sections
- **Student Identified**: During manual inspection of generations
- **Fix**: Added validation and fallback extraction logic

---

## 12. Compliance with Assessment Requirements

### 12.1 AI Usage Boundaries

**Permitted Uses** (as approved by lecturer):
- âœ… Code implementation and debugging
- âœ… Unit test generation
- âœ… Error message clarification
- âœ… Library behaviour explanation
- âœ… Refactoring suggestions

**Excluded Uses** (student work only):
- âœ… Research design and literature review
- âœ… Evaluation protocol development
- âœ… Metric selection and rationale
- âœ… Model selection and research questions
- âœ… Report writing and analysis
- âœ… Clinical interpretation and validation

### 12.2 Transparency Measures

1. **This Document**: Comprehensive log of all AI usage
2. **Code Comments**: All AI-generated code marked with comments
3. **Git History**: Shows progressive development and student review
4. **Viva Preparation**: Student can explain and defend all code and decisions

---

## 13. Statement of AI-Assisted Development & Logic Resolution

### 13.1 High-Level Strategy

An LLM assistant was utilised throughout this project in the role of a **Technical Supervisor** and **Code Generator**. The primary objective was to accelerate the implementation of the evaluation harness whilst maintaining strict human oversight over scientific definitions, metric validity, and failure mode interpretation. No AI was used to generate the final experimental data or results; AI was restricted to pipeline construction and debugging.

All code updates were **manually reviewed and accepted** by the student before integration, ensuring the student maintains full epistemic agency and responsibility for the implementation.

### 13.2 Key Areas of AI Integration

- **Metric Formalisation**: Translating conceptual definitions (e.g., "Faithfulness Gap measures if reasoning is functional") into executable mathematical formulas matching the LaTeX specification.
- **Pipeline Architecture**: Designing the modular Python structure (`extraction.py` â†’ `metrics.py` â†’ `calculate_metrics.py`) to ensure reproducibility and alignment with evaluation protocol.
- **Error Diagnosis**: Analysing raw JSONL logs to identify why specific models (like `gpt-oss-20b`) were failing evaluation steps, identifying extraction failures, and proposing deterministic solutions.

### 13.3 Critical Logic Resolution (The "Thinking" Part)

This section demonstrates how the student and AI collaborated to solve specific scientific problems, with the student maintaining decision-making authority.

#### Case Study A: The Extraction Crisis (Deterministic vs. Probabilistic)

**The Problem**: Initial regex-based extraction failed for verbose models like `gpt-oss-20b`, yielding 0.0 accuracy. Models were producing long-form responses with diagnoses buried in extensive reasoning text, making simple substring matching unreliable.

**AI Proposal 1**: The AI initially suggested using a secondary NLI model (AI-as-a-Judge) to extract diagnoses from the noisy text by asking "What is the diagnosis?" and parsing the NLI model's response.

**Student Logic Resolution**: The student rejected this approach because:
1. It introduces a "black box" variable that hurts reproducibility
2. Adds computational cost and latency
3. Introduces probabilistic noise (NLI models can disagree)
4. Violates the black-box evaluation principle (no secondary models)

**Final Methodology**: The student instead specified a **Closed-Set Matching** algorithm. Since the domain is bounded (Gold Labels are known from OpenR1-Psy), the student designed a whitelist-based approach:
- Extract all valid diagnoses from gold labels
- Scan model outputs against this whitelist using normalised text matching
- Use abbreviation mapping (MDD â†’ Major Depressive Disorder)
- This solved the extraction failure deterministically without adding probabilistic noise

**Implementation**: The AI implemented `extract_diagnosis_closed_set()` in `src/reliable_clinical_benchmark/metrics/extraction.py` based on the student's specification, which successfully extracted diagnoses from verbose outputs like `gpt-oss-20b`.

**Verification**: The student manually audited 20 random `gpt-oss-20b` generations to verify that Closed-Set Matching correctly identified diagnoses buried in long-form text.

#### Case Study B: Refusal vs. Incompetence

**The Problem**: The model `psyllm-gml-local` failed to produce valid diagnoses 73% of the time but did not trigger standard refusal keywords (e.g., "I cannot", "I'm not able to"). Initial extraction logic treated all failures as incorrect answers, potentially penalising safety-conscious models.

**Student Logic Resolution**: Through dialogue with the AI assistant, the student refined the distinction between:
- **Safety Refusal** (intentional withholding): Model explicitly refuses due to safety concerns, ethical boundaries, or insufficient information
- **Capability Failure** (hallucination/non-adherence): Model attempts to answer but produces invalid, hallucinated, or non-adherent responses

**Final Methodology**:
- **Refusals** (`is_refusal=True`) are **excluded** from accuracy calculations (to avoid penalising safety)
- **Extraction Failures** (Capability failures) are **penalised** as incorrect answers
- Refusal detection operates only on `clean_text` (stripping `<think>` tags) to prevent false positives where models discuss refusal during their reasoning chain but ultimately answer

**Implementation**: The AI implemented `is_refusal()` function in `src/reliable_clinical_benchmark/metrics/extraction.py` with the student's specified logic:
- Checks for explicit refusal phrases in cleaned text
- Excludes reasoning sections to avoid false positives
- Returns boolean for exclusion from accuracy calculations

**Verification**: The student validated refusal detection on known refusal cases and confirmed that safety-conscious models are not penalised for appropriate refusals.

#### Case Study C: Defining "Complexity" Metrics

**The Problem**: A raw "complexity" score conflated useful detail (length) with formatting errors (broken Unicode, special characters). Initial implementation used simple word count, which failed to distinguish between:
- Verbose but readable responses (high word count, good formatting)
- Concise but noisy responses (low word count, broken Unicode)

**Student Logic Resolution**: The student deconstructed the metric into two orthogonal signals:
1. **`response_verbosity`**: Log-scale word count to measure "time to read" (useful for clinical interpretability)
2. **`format_noise_score`**: Density of non-ASCII characters to measure "technical readability" (identifies formatting issues)

This separation allows the benchmark to distinguish between models that are verbose but readable versus models that produce technically problematic outputs.

**Implementation**: The AI implemented `compute_complexity_metrics()` in `src/reliable_clinical_benchmark/metrics/extraction.py` with:
- Log-scale verbosity: $\log(1 + \text{word\_count})$ to handle extreme outliers
- Format noise: Ratio of non-ASCII characters to total characters
- Both metrics reported separately for interpretability

**Verification**: The student tested on edge cases (1,500-word responses, responses with broken Unicode) and confirmed the metrics correctly distinguish verbosity from noise.

### 13.4 Verification Framework

To ensure the AI-generated code was functioning correctly, the following verification steps were taken:

1. **Unit Testing**: The extraction logic was tested against known edge cases (e.g., 1,500-word responses, models with broken Unicode, refusal cases). All tests were AI-generated but student-specified and validated.

2. **Manual Audit**: A random sample of 20 `gpt-oss-20b` generations was manually reviewed by the student to verify that Closed-Set Matching correctly identified the diagnosis buried in the text.

3. **Constraint Checking**: The refusal detection logic was constrained to operate only on `clean_text` (stripping `<think>` tags) to prevent false positives where models discussed refusal during their reasoning chain but ultimately answered.

4. **Mathematical Verification**: All metric calculations were verified against the LaTeX specification (`docs/Guides/Metrics and Evaluation.tex`) to ensure formula correctness.

5. **Clinical Validation**: Outputs were validated for clinical interpretabilityâ€”metrics must produce numbers that clinicians and regulators can understand.

### 13.5 Epistemic Agency Statement

The student treated the AI as a **junior developer**. The student defined:
- **Requirements** (e.g., "we need to stop the model from failing on long text")
- **Constraints** (e.g., "the metric must be deterministic, no secondary models")
- **Scientific decisions** (e.g., "refusals should be excluded from accuracy, not penalised")

The AI wrote the Python implementation, but the student:
- Audited the logic to ensure it matched scientific requirements
- Validated against evaluation protocol
- Made final decisions on methodology
- Maintained full responsibility for correctness

This demonstrates that the student remained in control of the **Science**, whilst the AI handled the **Syntax**.

---

## 14. Conclusion

AI tools were used **exclusively for engineering and implementation tasks**, enabling the student to focus on research design, validation, and ensuring clinical correctness. All research decisions, architecture design, and theoretical framework development were student-led. The AI usage log demonstrates:

1. **Clear boundaries**: AI used only for code, not research
2. **Student leadership**: All design and validation decisions made by student
3. **Quality assurance**: Comprehensive review and validation of all AI-generated code
4. **Transparency**: Complete documentation of AI usage with specific examples

The project demonstrates the student's ability to:
- Design and execute a complex research evaluation framework
- Translate mathematical specifications into working code
- Identify gaps and improve implementations
- Validate code correctness and clinical relevance
- Lead a development process whilst leveraging AI for productivity

---

---

## 14. Conclusion

This document demonstrates the student's ability to:
- Design and execute a complex research evaluation framework
- Translate mathematical specifications into working code
- Identify gaps and improve implementations
- Validate code correctness and clinical relevance
- Lead a development process whilst leveraging AI for productivity
- Maintain epistemic agency through critical logic resolution (Section 13)

The comprehensive case studies in Section 13 demonstrate that the student remained the scientific decision-maker throughout the project, using AI as a technical consultant and accelerator rather than a replacement for scientific reasoning. All code was manually reviewed and accepted, ensuring full student liability and responsibility for the implementation.

The project demonstrates that AI tools can be effectively used to accelerate engineering tasks whilst maintaining scientific rigour and student ownership of research decisions.

---

**Document Version**: 1.0  
**Last Updated**: January 2026  
**Student**: Ryan Mutiga Gichuru  
**Module**: CSY3055 Natural Language Processing

