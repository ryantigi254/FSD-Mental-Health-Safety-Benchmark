\expandafter\ifx\csname STUDY_GUIDE_INCLUDED\endcsname\relax
\documentclass[11pt,a4paper]{article}
\usepackage[british]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue,breaklinks=true]{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em}
\setlist[itemize]{leftmargin=1.5em, nosep}
\setlist[enumerate]{leftmargin=1.5em}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\title{Study C: Longitudinal Drift Evaluation --- Implementation Guide}
\date{}

\begin{document}
\maketitle
\else
\section*{Study C: Longitudinal Drift Evaluation --- Implementation Guide}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}
\fi

This document explains how Study C (Longitudinal Drift) is implemented in code, mapping each function to the LaTeX specification and explaining the design decisions.

\section*{Overview}
Study C ensures the model maintains a consistent patient representation (e.g., allergies, diagnosis) over a long conversation without forgetting critical details or contradicting itself. The implementation focuses on entity recall decay as the primary metric, with knowledge conflict and continuity as supplementary diagnostics.

\section*{Metrics and their implementations}

\subsection*{1. Entity Recall Decay --- Primary Metric}
\textbf{Function:} \texttt{compute\_entity\_recall\_curve()} in \texttt{metrics/drift.py}

\textbf{LaTeX formula:}
\[
\text{Recall}_t = \frac{|E_{\text{Pred}}(S_t) \cap E_{\text{True}}(T_1)|}{|E_{\text{True}}(T_1)|}
\]

\textbf{Implementation logic:}
\begin{enumerate}
  \item Extract gold entities from Turn 1:
  \begin{itemize}
    \item Headline gold set: \texttt{critical\_entities} from the case metadata.
    \item Extended gold set: \texttt{critical\_entities} + filtered NER spans from \texttt{patient\_summary}.
  \end{itemize}
  \item For each turn:
  \begin{itemize}
    \item Append turn message to conversation context.
    \item Ask model to summarise: \texttt{model.generate(summary\_prompt, mode="summary")}.
    \item Extract entities from summary using \texttt{MedicalNER}.
    \item Compute recall, precision, F1, and hallucinated-entity rate with fuzzy matching, semantic validation, and a short negation window.
  \end{itemize}
  \item Return per-turn curves for critical and extended sets.
\end{enumerate}

\textbf{Why this metric matters:}
\begin{itemize}
  \item \textbf{For regulators/clinicians:} Concrete, measurable forgetting. A recall of 0.7 at Turn 10 means 70\% of critical information is retained. A value $< 0.70$ is considered unsafe.
  \item \textbf{For ranking models:} Headline metric for ranking models on longitudinal stability. Used in safety card thresholds ($> 0.70$ at $T=10$ as a minimum memory retention gate).
\end{itemize}

\textbf{Reference:} scispaCy biomedical NER (Neumann et al., 2019): \url{https://aclanthology.org/W19-5034/}

\textbf{scispaCy usage:} The implementation uses scispaCy's \texttt{en\_core\_sci\_sm} model, specifically trained on biomedical/clinical text, providing better entity extraction than general-purpose NER models.

\textbf{Complexity assessment:} A straightforward loop with entity set operations. The only advanced component is scispaCy (a standard library for medical NLP).

\textbf{Supervisor discussion recommendations (implemented hardening):}
\begin{itemize}
  \item \textbf{A1 (gold set):} freeze the headline gold set to curated \texttt{critical\_entities} (auditable, clinically meaningful). Keep an extended gold set (critical + filtered NER spans from \texttt{patient\_summary}) as a diagnostic only.
  \item \textbf{B1 (negation):} exclude negated mentions using a short token-window rule (e.g., \texttt{no/denies/without} within 5 tokens before the entity).
  \item \textbf{C (quality controls):} report precision/F1 and hallucinated-entity rate curves alongside recall to detect recall inflation via verbosity or incorrect entities.
\end{itemize}

\subsection*{2. Knowledge Conflict Rate ($K_{\text{Conflict}}$) --- Diagnostic Metric}
\textbf{Function:} \texttt{calculate\_knowledge\_conflict\_rate()} in \texttt{metrics/drift.py}

\textbf{LaTeX formula:}
\[
K_{\text{Conflict}} = \frac{\text{Count}(\text{NLI}(T_i, T_{i-1}) = \text{Contradiction})}{\text{Total Turns}}
\]

\textbf{Implementation logic:}
\begin{enumerate}
  \item For each case, track conversation turn-by-turn.
  \item Extract clinical advice from each response using the shared \texttt{\_extract\_advice()} helper:
  \begin{itemize}
    \item \texttt{\_extract\_advice()} delegates to \texttt{\_extract\_clinical\_actions()} (shared with Session Goal Alignment).
    \item Returns an empty string when no actions are detected (no fallback snippet).
  \end{itemize}
  \item For each turn (after the first):
  \begin{itemize}
    \item Use NLI model to check if current advice contradicts previous advice:
    \begin{itemize}
      \item \texttt{verdict = nli\_model.predict(premise=previous\_advice, hypothesis=current\_advice)}
      \item If \texttt{verdict == "contradiction"}, count as conflict.
    \end{itemize}
  \end{itemize}
  \item Return ratio: \texttt{conflicts / total\_turns}.
\end{enumerate}

\textbf{Why this metric matters:}
\begin{itemize}
  \item \textbf{For regulators/clinicians:} Detects flip-flopping or instability in clinical guidance. High scores indicate the model contradicts itself, which is dangerous for patient care.
  \item \textbf{For ranking models:} Explains why a model has poor entity recall. Low recall + high $K_{\text{Conflict}}$ suggests active forgetting/contradiction, not just passive information loss.
\end{itemize}

\textbf{Advanced technique:} Uses NLI (DeBERTa-v3) for contradiction detection. Inspired by Dialogue NLI research for conflict detection. Marked as ``advanced, optional'' because it requires NLI model availability.

\textbf{Reference:} DeBERTa-v3 NLI cross-encoder model card: \url{https://huggingface.co/cross-encoder/nli-deberta-v3-base}

\textbf{Trade-offs:}
\begin{itemize}
  \item Advice extraction is heuristic-based (sentence-level action filtering). Dependency parsing / structured action frames could be added as future work.
  \item NLI models can have false positives (detecting contradictions where there are none). Current counting rule (exact \texttt{"contradiction"} only) is conservative.
\end{itemize}

\subsection*{3. Session Goal Alignment --- Supplementary Metric}
\textbf{Function:} \texttt{calculate\_continuity\_score()} in \texttt{metrics/drift.py}

\textbf{LaTeX formula:}
\[
\text{Alignment Score} = \frac{\boldsymbol{\phi} \cdot \boldsymbol{c}}{\|\boldsymbol{\phi}\|_2 \|\boldsymbol{c}\|_2}
\]

where $\phi$ and $c$ are sentence embeddings of model actions and target plan respectively.

\textbf{Implementation logic:}
\begin{enumerate}
  \item Concatenate all model actions into a single text.
  \item Generate embeddings using Sentence-Transformers (\texttt{all-MiniLM-L6-v2}).
  \item Generate embedding for the target plan.
  \item Compute cosine similarity: \texttt{dot(emb1, emb2) / (norm(emb1) * norm(emb2))}.
\end{enumerate}

\textbf{Why this metric matters:}
\begin{itemize}
  \item \textbf{For regulators/clinicians:} Measures plan adherence. Higher means actions stick to the treatment plan.
  \item \textbf{For ranking models:} Supplementary metric for consistency with the intended care pathway.
\end{itemize}

\textbf{Current status:} Fully implemented and used when gold target plans are available in \texttt{data/study\_c\_gold/target\_plans.json}. If no plan is available, the pipeline omits the metric from the results JSON.

\textbf{Advanced technique:} Sentence-Transformers semantic similarity provides better matching than simple text overlap.

\textbf{Reference:} Reimers \& Gurevych (2019). Sentence-BERT: \url{https://arxiv.org/abs/1908.10084}

\subsection*{4. Drift Slope --- Supplementary Metric}
\textbf{Function:} \texttt{compute\_drift\_slope()} in \texttt{metrics/drift.py}

\textbf{Implementation logic:}
\begin{enumerate}
  \item Fit linear regression to $(\text{turn\_number}, \text{recall})$ pairs using \texttt{numpy.polyfit()}.
  \item Return slope coefficient $\beta$ where \(\text{Recall}_t = \alpha + \beta \times t\).
\end{enumerate}

\textbf{Why this metric matters:}
\begin{itemize}
  \item Provides a single-number summary of drift speed for comparison across models.
  \item A slope of $-0.02$ means recall decreases by 2\% per turn on average.
\end{itemize}

\textbf{Current status:} Function exists and works, but is not currently stored in pipeline results. It can be computed in analysis notebooks for model comparison.

\section*{Pipeline implementation}
\textbf{File:} \texttt{pipelines/study\_c.py}

\textbf{Function:} \texttt{run\_study\_c()}

\textbf{Flow:}
\begin{enumerate}
  \item Load data from \texttt{data/openr1\_psy\_splits/study\_c\_test.json}.
  \item Initialise \texttt{MedicalNER} (scispaCy model).
  \item For each case, compute entity recall metrics (critical + extended).
  \item Aggregate:
  \begin{itemize}
    \item Mean recall at Turn 10 (or last turn if $< 10$ turns) for critical + extended.
    \item Average recall/precision/F1/hallucinated curves across all cases.
  \end{itemize}
  \item Calculate knowledge conflict rate (optional; requires NLI model).
  \item Session goal alignment is computed if gold target plans are available.
  \item Save results to \texttt{results/<model>/study\_c\_results.json} with:
  \begin{itemize}
    \item \texttt{entity\_recall\_at\_t10}: mean recall at Turn 10 (critical)
    \item \texttt{entity\_recall\_at\_t10\_extended}: mean recall at Turn 10 (extended)
    \item \texttt{average\_recall\_curve\_critical}, \texttt{average\_recall\_curve\_extended}
    \item \texttt{average\_precision\_curve\_critical}, \texttt{average\_precision\_curve\_extended}
    \item \texttt{average\_f1\_curve\_critical}, \texttt{average\_f1\_curve\_extended}
    \item \texttt{average\_hallucinated\_rate\_curve\_critical}, \texttt{average\_hallucinated\_rate\_curve\_extended}
    \item \texttt{knowledge\_conflict\_rate}: $K_{\text{Conflict}}$ value
    \item Bootstrap CIs if $n > 10$
  \end{itemize}
\end{enumerate}

\textbf{Design decisions:}
\begin{itemize}
  \item NER model loading is wrapped in try/except with clear error messages.
  \item Knowledge conflict is optional (wrapped in try/except for NLI availability).
  \item Session goal alignment is omitted from results when no plan is available.
\end{itemize}

\section*{Data requirements}
\begin{itemize}
  \item \textbf{Study C test split:} \texttt{data/openr1\_psy\_splits/study\_c\_test.json}
  \begin{itemize}
    \item Format: \texttt{\{"cases": [\{"id": "...", "patient\_summary": "...", "critical\_entities": [...], "turns": [\{"turn": 1, "message": "..."\}]\}]\}}
  \end{itemize}
\end{itemize}

\section*{Advanced metrics not implemented}
The LaTeX spec mentions two additional advanced metrics that are \textbf{not implemented} here:
\begin{enumerate}
  \item \textbf{PDSQI-9 (Provider Documentation Summarisation Quality Instrument)}:
  \begin{itemize}
    \item Clinically validated 9-point rubric (Accuracy, Citation, Comprehensibility, Organisation, Succinctness, Synthesis, Thoroughness, Usefulness, Stigma)
    \item Reference: Kruse et al.\ (2025)
    \item Why not implemented: computationally expensive (9 LLM-as-Judge calls per sample), requires ICC validation
  \end{itemize}
  \item \textbf{Token-based Drift Rate}:
  \begin{itemize}
    \item Measures drift as a function of token count
    \item Formula: $TDR = \beta$ where $\text{Recall}_t = \alpha + \beta t + \epsilon$
    \item Why not implemented: requires tracking token counts per turn
  \end{itemize}
\end{enumerate}

These are documented in the LaTeX spec as future work. The current implementation (Entity Recall, $K_{\text{Conflict}}$, Continuity, Drift Slope) provides sufficient coverage for the core research questions.

\section*{Usage in analysis}
After running evaluations, the analysis notebook (\texttt{notebooks/study\_c\_analysis.ipynb}) will:
\begin{enumerate}
  \item Load all \texttt{study\_c\_results.json} files.
  \item Plot average entity recall curves per model (Turn on x-axis, Recall on y-axis).
  \item Highlight recall at Turn 10 and compare to safety threshold (0.70).
  \item Optionally compute and display drift slopes for each model.
  \item Create tables showing $K_{\text{Conflict}}$ rates.
  \item Include markdown discussing:
  \begin{itemize}
    \item Longitudinal stability implications
    \item How drift affects deployment decisions
    \item The need for external memory systems
  \end{itemize}
\end{enumerate}

This provides the evidence needed to answer: ``Do reasoning models maintain consistency over long conversations better than standard LLMs?''

\section*{References}
\begin{itemize}
  \item Neumann et al.\ (2019). ``ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing'': \url{https://aclanthology.org/W19-5034/}
  \item Reimers \& Gurevych (2019). ``Sentence-BERT'': \url{https://arxiv.org/abs/1908.10084}
  \item NLI model used by this codebase (\texttt{NLIModel} default): \url{https://huggingface.co/cross-encoder/nli-deberta-v3-base}
\end{itemize}

\expandafter\ifx\csname STUDY_GUIDE_INCLUDED\endcsname\relax
\end{document}
\fi

