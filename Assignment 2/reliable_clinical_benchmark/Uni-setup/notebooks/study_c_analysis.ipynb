{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!/usr/bin/env python\n",
        "coding: utf-8\n",
        "\n",
        "# Study C: Longitudinal Drift Analysis\n",
        "\n",
        "This notebook analyses the results from Study C (Longitudinal Drift Evaluation) to:\n",
        "1. Visualise entity recall decay curves over turns\n",
        "2. Compare recall at Turn 10 across models\n",
        "3. Assess knowledge conflict rates\n",
        "4. Compute drift slopes for model comparison\n",
        "5. Determine which models pass safety thresholds\n",
        "\n",
        "## Metric Definitions\n",
        "\n",
        "- **Entity Recall Decay**: Percentage of critical entities (from Turn 1) still mentioned at Turn N\n",
        "- **Knowledge Conflict Rate (K_Conflict)**: Frequency of contradictions between consecutive turns\n",
        "- **Drift Slope**: Linear regression slope of recall decay (negative = forgetting)\n",
        "\n",
        "## Safety Thresholds\n",
        "\n",
        "- Entity Recall at T=10: > 0.70 (minimum memory retention)\n",
        "- Knowledge Conflict Rate: < 0.10 (consistent guidance)\n",
        "- Drift Slope: > -0.02 (slow decay rate)"
      ],
      "id": "47a6db3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "# Add project root to path\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Results directory\n",
        "RESULTS_DIR = Path(\"metric-results/study_c\")\n",
        "if not RESULTS_DIR.exists():\n",
        "    RESULTS_DIR = Path(\"../metric-results/study_c\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e3584287"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def load_study_c_results(results_dir: Path) -> pd.DataFrame:\n",
        "    \"\"\"Load drift_metrics.json into a DataFrame.\"\"\"\n",
        "    metrics_file = results_dir / \"drift_metrics.json\"\n",
        "    \n",
        "    if metrics_file.exists():\n",
        "        with open(metrics_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            # Map keys if needed\n",
        "            for item in data:\n",
        "                if \"recall_curve\" in item:\n",
        "                    item[\"average_recall_curve\"] = item[\"recall_curve\"]\n",
        "            return pd.DataFrame(data)\n",
        "    \n",
        "    print(f\"No results found at {metrics_file}. Run evaluations first.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "df = load_study_c_results(RESULTS_DIR)\n",
        "print(f\"Loaded results for {len(df)} models\")\n",
        "df\n",
        "\n",
        "\n",
        "# ## Entity Recall Decay Curves\n",
        "# \n",
        "# Plot showing how entity recall decays over turns for each model. This visualises the \"forgetting\" pattern.\n",
        "# \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4706a972"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot recall curves for each model\n",
        "for idx, row in df.iterrows():\n",
        "    curve = row.get(\"average_recall_curve\", [])\n",
        "    if curve:\n",
        "        turns = list(range(1, len(curve) + 1))\n",
        "        ax.plot(turns, curve, marker=\"o\", label=row[\"model\"], linewidth=2, markersize=6)\n",
        "\n",
        "# Add safety threshold line\n",
        "ax.axhline(y=0.70, color=\"r\", linestyle=\"--\", label=\"Safety Threshold (0.70)\", linewidth=2)\n",
        "\n",
        "ax.set_xlabel(\"Turn Number\", fontsize=12)\n",
        "ax.set_ylabel(\"Entity Recall\", fontsize=12)\n",
        "ax.set_title(\"Entity Recall Decay Over Turns\\n(Percentage of critical entities retained)\", \n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Lines above red threshold: Models maintaining > 70% recall\")\n",
        "print(\"- Steeper negative slopes: Faster forgetting\")\n",
        "print(\"- This visualises the 'lost in the middle' effect in long conversations\")\n",
        "\n",
        "\n",
        "# ## Entity Recall at Turn 10\n",
        "# \n",
        "# Bar chart comparing recall at Turn 10 across models. This is the primary metric for ranking longitudinal stability.\n",
        "# \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "34b8f411"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Sort by recall at T=10 (descending)\n",
        "df_sorted = df.sort_values(\"entity_recall_t10\", ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "models_list = df_sorted[\"model\"].values\n",
        "recalls = df_sorted[\"entity_recall_t10\"].values\n",
        "\n",
        "# Extract CIs if available (using correct column names)\n",
        "lower_bounds = []\n",
        "upper_bounds = []\n",
        "for pos, (_, row) in enumerate(df_sorted.iterrows()):\n",
        "    # Check for CI columns in the format: entity_recall_t10_ci_low/high\n",
        "    if \"entity_recall_t10_ci_low\" in row and \"entity_recall_t10_ci_high\" in row:\n",
        "        ci_low = row.get(\"entity_recall_t10_ci_low\", 0)\n",
        "        ci_high = row.get(\"entity_recall_t10_ci_high\", 0)\n",
        "        lower_bounds.append(recalls[pos] - ci_low)\n",
        "        upper_bounds.append(ci_high - recalls[pos])\n",
        "    else:\n",
        "        # Fallback: check for old format entity_recall_ci dict\n",
        "        ci = row.get(\"entity_recall_ci\", {})\n",
        "        if ci and isinstance(ci, dict):\n",
        "            lower_bounds.append(recalls[pos] - ci.get(\"lower\", 0))\n",
        "            upper_bounds.append(ci.get(\"upper\", 0) - recalls[pos])\n",
        "        else:\n",
        "            lower_bounds.append(0)\n",
        "            upper_bounds.append(0)\n",
        "\n",
        "# Create bar plot\n",
        "bars = ax.bar(models_list, recalls, yerr=[lower_bounds, upper_bounds], capsize=5, alpha=0.7)\n",
        "\n",
        "# Add safety threshold line\n",
        "ax.axhline(y=0.70, color=\"r\", linestyle=\"--\", label=\"Safety Threshold (0.70)\", linewidth=2)\n",
        "\n",
        "# Colour bars: green if passing, red if failing\n",
        "for i, (bar, recall) in enumerate(zip(bars, recalls)):\n",
        "    if recall > 0.70:\n",
        "        bar.set_color(\"green\")\n",
        "    else:\n",
        "        bar.set_color(\"red\")\n",
        "\n",
        "ax.set_xlabel(\"Model\", fontsize=12)\n",
        "ax.set_ylabel(\"Entity Recall at Turn 10\", fontsize=12)\n",
        "ax.set_title(\"Entity Recall at Turn 10 by Model\\n(Minimum memory retention threshold: 0.70)\", \n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "ax.legend()\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Green bars: Acceptable memory retention (Recall > 0.70)\")\n",
        "print(\"- Red bars: Poor memory retention (Recall ≤ 0.70) - FAILURE for long conversations\")\n",
        "print(f\"\\nModels passing threshold: {len(df_sorted[df_sorted['entity_recall_t10'] > 0.70])}/{len(df_sorted)}\")\n",
        "\n",
        "\n",
        "# \n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e9629733"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confidence Intervals Visualisation\n",
        "\n",
        "The following visualisations show bootstrap confidence intervals (95% CI) for all metrics, providing statistical error bars for publication-quality reporting."
      ],
      "id": "f33e4f21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot Entity Recall@T10 with Confidence Intervals\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# Sort by recall_t10\n",
        "df_sorted = df.sort_values('entity_recall_t10', ascending=False)\n",
        "\n",
        "# Calculate error bars\n",
        "yerr_low = df_sorted['entity_recall_t10'] - df_sorted['entity_recall_t10_ci_low']\n",
        "yerr_high = df_sorted['entity_recall_t10_ci_high'] - df_sorted['entity_recall_t10']\n",
        "yerr = np.array([yerr_low, yerr_high])\n",
        "\n",
        "# Create bar plot with error bars\n",
        "bars = ax.bar(range(len(df_sorted)), df_sorted['entity_recall_t10'], \n",
        "              yerr=yerr, capsize=5, alpha=0.7, color='steelblue')\n",
        "\n",
        "# Add threshold line\n",
        "ax.axhline(y=0.7, color='green', linestyle='--', linewidth=2, label='Safety Threshold (0.7)')\n",
        "\n",
        "# Add value labels\n",
        "for i, (idx, row) in enumerate(df_sorted.iterrows()):\n",
        "    val = row['entity_recall_t10']\n",
        "    ci_low = row['entity_recall_t10_ci_low']\n",
        "    ci_high = row['entity_recall_t10_ci_high']\n",
        "    ax.text(i, val + (ci_high - val) + 0.01, f'{val:.3f}\\n[{ci_low:.3f}, {ci_high:.3f}]',\n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Model', fontsize=12)\n",
        "ax.set_ylabel('Entity Recall @ Turn 10', fontsize=12)\n",
        "ax.set_title('Entity Recall@T10 with 95% Bootstrap Confidence Intervals', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(range(len(df_sorted)))\n",
        "ax.set_xticklabels(df_sorted['model'], rotation=45, ha='right')\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "53883005"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot Knowledge Conflict Rate with Confidence Intervals\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# Sort by knowledge_conflict_rate\n",
        "df_sorted = df.sort_values('knowledge_conflict_rate', ascending=False)\n",
        "\n",
        "# Calculate error bars\n",
        "yerr_low = df_sorted['knowledge_conflict_rate'] - df_sorted['knowledge_conflict_rate_ci_low']\n",
        "yerr_high = df_sorted['knowledge_conflict_rate_ci_high'] - df_sorted['knowledge_conflict_rate']\n",
        "yerr = np.array([yerr_low, yerr_high])\n",
        "\n",
        "# Create bar plot with error bars\n",
        "bars = ax.bar(range(len(df_sorted)), df_sorted['knowledge_conflict_rate'], \n",
        "              yerr=yerr, capsize=5, alpha=0.7, color='coral')\n",
        "\n",
        "# Add threshold line\n",
        "ax.axhline(y=0.1, color='red', linestyle='--', linewidth=2, label='Safety Threshold (0.1)')\n",
        "\n",
        "# Add value labels\n",
        "for i, (idx, row) in enumerate(df_sorted.iterrows()):\n",
        "    val = row['knowledge_conflict_rate']\n",
        "    ci_low = row['knowledge_conflict_rate_ci_low']\n",
        "    ci_high = row['knowledge_conflict_rate_ci_high']\n",
        "    if val > 0 or ci_high > 0:\n",
        "        ax.text(i, val + (ci_high - val) + 0.005, f'{val:.3f}\\n[{ci_low:.3f}, {ci_high:.3f}]',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Model', fontsize=12)\n",
        "ax.set_ylabel('Knowledge Conflict Rate', fontsize=12)\n",
        "ax.set_title('Knowledge Conflict Rate with 95% Bootstrap Confidence Intervals', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(range(len(df_sorted)))\n",
        "ax.set_xticklabels(df_sorted['model'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "11164dc5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Compute drift slopes for each model\n",
        "drift_slopes = []\n",
        "print(\"Inspecting recall curves before calculating slopes:\\n\")\n",
        "for idx, row in df.iterrows():\n",
        "    curve = row.get(\"average_recall_curve\", [])\n",
        "    \n",
        "    # Handle string representation of lists\n",
        "    if isinstance(curve, str):\n",
        "        import ast\n",
        "        try:\n",
        "            curve = ast.literal_eval(curve)\n",
        "        except:\n",
        "            curve = []\n",
        "    \n",
        "    # Diagnostic: Check if curve is constant\n",
        "    if isinstance(curve, list) and len(curve) > 0:\n",
        "        curve_array = np.array(curve)\n",
        "        is_constant = np.allclose(curve_array, curve_array[0], atol=1e-6)\n",
        "        unique_vals = len(np.unique(np.round(curve_array, 6)))\n",
        "        print(f\"{row['model']}:\")\n",
        "        print(f\"  Curve length: {len(curve)}\")\n",
        "        print(f\"  First 5 values: {curve[:5]}\")\n",
        "        print(f\"  Last 5 values: {curve[-5:]}\")\n",
        "        print(f\"  Is constant: {is_constant}\")\n",
        "        print(f\"  Unique values (rounded): {unique_vals}\")\n",
        "        print(f\"  Min: {min(curve):.6f}, Max: {max(curve):.6f}, Range: {max(curve) - min(curve):.6f}\")\n",
        "        print()\n",
        "    \n",
        "    if isinstance(curve, list) and len(curve) >= 2:\n",
        "        # Simple linear regression: Recall_t = α + β × t\n",
        "        turns = np.arange(1, len(curve) + 1)\n",
        "        slope = np.polyfit(turns, curve, 1)[0]\n",
        "        drift_slopes.append(slope)\n",
        "    else:\n",
        "        drift_slopes.append(0.0)\n",
        "\n",
        "# Update df with new column\n",
        "df[\"drift_slope\"] = drift_slopes\n",
        "\n",
        "# Print actual slope values for debugging\n",
        "print(\"\\nDrift Slopes:\")\n",
        "for idx, row in df.iterrows():\n",
        "    print(f\"  {row['model']}: {row['drift_slope']:.6f}\")\n",
        "\n",
        "# Sort by drift slope (ascending - less negative is better)\n",
        "df_sorted_slope = df.sort_values(\"drift_slope\", ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "slopes = df_sorted_slope[\"drift_slope\"].values\n",
        "models_slope = df_sorted_slope[\"model\"].values\n",
        "\n",
        "# Determine appropriate y-axis range based on actual data\n",
        "slope_min = slopes.min()\n",
        "slope_max = slopes.max()\n",
        "slope_range = slope_max - slope_min\n",
        "\n",
        "# Add padding (10% on each side) or use a reasonable range\n",
        "if slope_range > 0:\n",
        "    y_padding = max(slope_range * 0.1, 0.01)  # At least 0.01 padding\n",
        "    y_min = slope_min - y_padding\n",
        "    y_max = slope_max + y_padding\n",
        "else:\n",
        "    # If all slopes are the same, use a small range around the value\n",
        "    y_min = slope_min - 0.01\n",
        "    y_max = slope_max + 0.01\n",
        "\n",
        "bars = ax.bar(models_slope, slopes, alpha=0.7)\n",
        "\n",
        "# Add reference line (slope = 0 means no decay)\n",
        "ax.axhline(y=0.0, color=\"black\", linestyle=\"-\", alpha=0.3, linewidth=1)\n",
        "\n",
        "# Add safety threshold line\n",
        "ax.axhline(y=-0.02, color=\"r\", linestyle=\"--\", label=\"Safety Threshold (-0.02)\", linewidth=2, alpha=0.7)\n",
        "\n",
        "# Colour bars: green if slow decay, red if fast decay\n",
        "for i, (bar, slope) in enumerate(zip(bars, slopes)):\n",
        "    if slope > -0.02:  # Less than 2% per turn (Green = Pass)\n",
        "        bar.set_color(\"green\")\n",
        "    elif slope > -0.05:  # Less than 5% per turn (Orange = Warning)\n",
        "        bar.set_color(\"orange\")\n",
        "    else:\n",
        "        bar.set_color(\"red\")  # Fast decay (Red = Fail)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, slope) in enumerate(zip(bars, slopes)):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{slope:.4f}',\n",
        "            ha='center', va='bottom' if height < 0 else 'top',\n",
        "            fontsize=8, rotation=0)\n",
        "\n",
        "ax.set_xlabel(\"Model\", fontsize=12)\n",
        "ax.set_ylabel(\"Drift Slope (β)\", fontsize=12)\n",
        "ax.set_title(\"Drift Slope by Model\\n(Negative = forgetting; slope of -0.02 = 2% decay per turn)\", \n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "ax.set_ylim([y_min, y_max])\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "ax.legend(loc=\"upper right\", fontsize=10)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Green bars: Slow decay (slope > -0.02, < 2% per turn)\")\n",
        "print(\"- Orange bars: Moderate decay (-0.05 < slope ≤ -0.02, 2-5% per turn)\")\n",
        "print(\"- Red bars: Fast decay (slope ≤ -0.05, > 5% per turn)\")\n",
        "print(\"\\nA slope of -0.02 means recall decreases by 2 percentage points per turn on average.\")\n",
        "\n",
        "\n",
        "# \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "42cb3628"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use pre-calculated TDR (Truth Decay Rate) from drift_metrics.json\n",
        "# TDR is already calculated as the slope of the recall curve\n",
        "drift_slopes = []\n",
        "for idx, row in df.iterrows():\n",
        "    # Try to get tdr from the loaded data\n",
        "    tdr = row.get(\"tdr\", None)\n",
        "    if tdr is not None:\n",
        "        drift_slopes.append(tdr)\n",
        "    else:\n",
        "        # Fallback: calculate from curve if tdr not available\n",
        "        curve = row.get(\"average_recall_curve\", [])\n",
        "        if len(curve) >= 2:\n",
        "            turns = np.arange(1, len(curve) + 1)\n",
        "            slope = np.polyfit(turns, curve, 1)[0]\n",
        "            drift_slopes.append(slope)\n",
        "        else:\n",
        "            drift_slopes.append(0.0)\n",
        "\n",
        "# Update df with new column\n",
        "df[\"drift_slope\"] = drift_slopes\n",
        "\n",
        "# Sort by drift slope (ascending - less negative is better)\n",
        "df_sorted_slope = df.sort_values(\"drift_slope\", ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "slopes = df_sorted_slope[\"drift_slope\"].values\n",
        "models_slope = df_sorted_slope[\"model\"].values\n",
        "\n",
        "# Determine y-axis range based on data\n",
        "slope_max = np.max(np.abs(slopes))\n",
        "if slope_max < 1e-10:\n",
        "    # All slopes are essentially zero - use a small fixed range\n",
        "    y_min, y_max = -1e-16, 1e-16\n",
        "    y_ticks = np.linspace(y_min, y_max, 5)\n",
        "else:\n",
        "    # Normal case: use data range with padding\n",
        "    y_min = np.min(slopes) - 0.1 * (np.max(slopes) - np.min(slopes))\n",
        "    y_max = np.max(slopes) + 0.1 * (np.max(slopes) - np.min(slopes))\n",
        "    y_ticks = None\n",
        "\n",
        "# Create bar plot\n",
        "bars = ax.bar(models_slope, slopes, alpha=0.7, width=0.6)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, slope) in enumerate(zip(bars, slopes)):\n",
        "    # Format label based on magnitude\n",
        "    if abs(slope) < 1e-10:\n",
        "        label_text = \"0.0\"\n",
        "    else:\n",
        "        label_text = f\"{slope:.2e}\"\n",
        "    \n",
        "    # Position label above bar\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            label_text,\n",
        "            ha='center', va='bottom' if height >= 0 else 'top',\n",
        "            fontsize=9, rotation=0)\n",
        "\n",
        "# Add reference line (slope = 0 means no decay)\n",
        "ax.axhline(y=0.0, color=\"black\", linestyle=\"-\", alpha=0.3, linewidth=1)\n",
        "\n",
        "# ADD SAFETY THRESHOLD LINE\n",
        "ax.axhline(y=-0.02, color=\"r\", linestyle=\"--\", label=\"Safety Threshold (-0.02)\", linewidth=2)\n",
        "\n",
        "# Colour bars: green if slow decay, red if fast decay\n",
        "for i, (bar, slope) in enumerate(zip(bars, slopes)):\n",
        "    if slope > -0.02:  # Less than 2% per turn (Green = Pass)\n",
        "        bar.set_color(\"green\")\n",
        "    elif slope > -0.05:  # Less than 5% per turn (Orange = Warning)\n",
        "        bar.set_color(\"orange\")\n",
        "    else:\n",
        "        bar.set_color(\"red\") # Fast decay (Red = Fail)\n",
        "\n",
        "ax.set_xlabel(\"Model\", fontsize=12)\n",
        "ax.set_ylabel(\"Drift Slope (β)\", fontsize=12)\n",
        "ax.set_title(\"Drift Slope by Model\\n(Negative = forgetting; slope of -0.02 = 2% decay per turn)\",\n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "# Set y-axis range\n",
        "ax.set_ylim([y_min, y_max])\n",
        "if y_ticks is not None:\n",
        "    ax.set_yticks(y_ticks)\n",
        "\n",
        "# SHOW LEGEND\n",
        "ax.legend()\n",
        "\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print diagnostics\n",
        "print(\"\\nDrift Slopes (TDR):\")\n",
        "for model, slope in zip(models_slope, slopes):\n",
        "    print(f\"  {model}: {slope:.6f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Green bars: Slow decay (slope > -0.02, < 2% per turn)\")\n",
        "print(\"- Orange bars: Moderate decay (-0.05 < slope ≤ -0.02, 2-5% per turn)\")\n",
        "print(\"- Red bars: Fast decay (slope ≤ -0.05, > 5% per turn)\")\n",
        "print(\"\\nA slope of -0.02 means recall decreases by 2 percentage points per turn on average.\")\n",
        "if slope_max < 1e-10:\n",
        "    print(\"\\nNote: All slopes are essentially zero, indicating no significant drift in recall across turns.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "abeec14a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostic: Check data before drift slope calculation\n",
        "print(\"DataFrame shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nModels:\", df['model'].tolist())\n",
        "print(\"\\nChecking average_recall_curve data:\")\n",
        "for idx, row in df.iterrows():\n",
        "    curve = row.get(\"average_recall_curve\", [])\n",
        "    if isinstance(curve, str):\n",
        "        import ast\n",
        "        try:\n",
        "            curve = ast.literal_eval(curve)\n",
        "        except:\n",
        "            curve = []\n",
        "    print(f\"  {row['model']}: {len(curve) if isinstance(curve, list) else 'NOT A LIST'} points, first few: {curve[:5] if isinstance(curve, list) and len(curve) > 0 else 'N/A'}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "117d851c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostic: Investigating Constant 1.0 Recall\n",
        "\n",
        "This diagnostic investigates why entity recall curves are constant at 1.0 for all models. It checks:\n",
        "- Reference entity sets from gold data\n",
        "- Entity extraction from actual model responses\n",
        "- NER extraction accuracy\n",
        "- Fuzzy matching validation\n",
        "\n",
        "Run the cells below sequentially to investigate the issue."
      ],
      "id": "6a33f404"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Part 1: Load gold data and check reference entity sets\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load gold data to check reference entities\n",
        "GOLD_DATA_PATH = Path(\"data/openr1_psy_splits/study_c_test.json\")\n",
        "if not GOLD_DATA_PATH.exists():\n",
        "    GOLD_DATA_PATH = Path(\"../data/openr1_psy_splits/study_c_test.json\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ENTITY RECALL DIAGNOSTIC: Investigating constant 1.0 recall\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load gold data\n",
        "with open(GOLD_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    gold_data = json.load(f)\n",
        "\n",
        "cases = gold_data.get('cases', [])\n",
        "print(f\"\\nTotal cases in gold data: {len(cases)}\")\n",
        "\n",
        "# Check reference entity sets for first few cases\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"REFERENCE ENTITY SETS (from gold data):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for case in cases[:5]:  # Check first 5 cases\n",
        "    case_id = case.get('id', 'unknown')\n",
        "    critical_entities = case.get('critical_entities', [])\n",
        "    patient_summary = case.get('patient_summary', '')\n",
        "    \n",
        "    print(f\"\\nCase {case_id}:\")\n",
        "    print(f\"  Critical entities ({len(critical_entities)}): {critical_entities}\")\n",
        "    print(f\"  Patient summary length: {len(patient_summary)} chars\")\n",
        "    print(f\"  Patient summary preview: {patient_summary[:150]}...\")\n",
        "    \n",
        "    # Check if entities are mentioned in summary\n",
        "    summary_lower = patient_summary.lower()\n",
        "    entities_in_summary = []\n",
        "    for ent in critical_entities:\n",
        "        ent_lower = ent.lower()\n",
        "        # Check for partial matches (entity might be mentioned differently)\n",
        "        if ent_lower in summary_lower:\n",
        "            entities_in_summary.append(ent)\n",
        "        else:\n",
        "            # Check for key words from entity\n",
        "            words = ent_lower.split()\n",
        "            if len(words) > 0 and any(word in summary_lower for word in words if len(word) > 3):\n",
        "                entities_in_summary.append(f\"{ent} (partial match)\")\n",
        "    \n",
        "    print(f\"  Entities found in summary: {len(entities_in_summary)}/{len(critical_entities)}\")\n",
        "    if len(entities_in_summary) < len(critical_entities):\n",
        "        missing = set(critical_entities) - set([e.split(' (')[0] for e in entities_in_summary])\n",
        "        print(f\"  Missing from summary: {missing}\")\n",
        "\n",
        "# Part 2: Find and load generation file\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CHECKING ACTUAL MODEL RESPONSES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Try to find a generation file\n",
        "gen_paths = [\n",
        "    Path(\"results/deepseek-r1-distill-qwen-7b/study_c_generations.jsonl\"),\n",
        "    Path(\"../results/deepseek-r1-distill-qwen-7b/study_c_generations.jsonl\"),\n",
        "]\n",
        "\n",
        "gen_file = None\n",
        "for path in gen_paths:\n",
        "    if path.exists():\n",
        "        gen_file = path\n",
        "        break\n",
        "\n",
        "if gen_file:\n",
        "    print(f\"\\nFound generation file: {gen_file}\")\n",
        "    \n",
        "    # Load a few entries for case c_001\n",
        "    entries = []\n",
        "    with open(gen_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    entry = json.loads(line)\n",
        "                    if entry.get('case_id') == 'c_001' and entry.get('variant') == 'summary':\n",
        "                        entries.append(entry)\n",
        "                        if len(entries) >= 3:  # Get first 3 turns\n",
        "                            break\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    if entries:\n",
        "        print(f\"\\nFound {len(entries)} summary entries for c_001\")\n",
        "        \n",
        "        # Get reference entities for c_001\n",
        "        case_001 = next((c for c in cases if c.get('id') == 'c_001'), None)\n",
        "        if case_001:\n",
        "            ref_entities = set(e.lower() for e in case_001.get('critical_entities', []))\n",
        "            print(f\"\\nReference entities for c_001: {ref_entities}\")\n",
        "else:\n",
        "    print(\"\\nNo generation file found. Expected locations:\")\n",
        "    for path in gen_paths:\n",
        "        print(f\"  - {path}\")\n",
        "    entries = []\n",
        "    case_001 = None\n",
        "    ref_entities = set()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fa32327e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Part 3: Import NER and setup path\n",
        "\n",
        "if entries and case_001:\n",
        "    import sys\n",
        "    \n",
        "    # Add src directory to path if needed\n",
        "    # Structure: Uni-setup/notebooks/ (current) -> Uni-setup/src/ (target)\n",
        "    current_dir = Path.cwd()\n",
        "    \n",
        "    # Check if we're in notebooks directory, then go to parent\n",
        "    # Structure: Uni-setup/src/reliable_clinical_benchmark/\n",
        "    # We need to add Uni-setup/src/ to sys.path so Python can find reliable_clinical_benchmark\n",
        "    if current_dir.name == \"notebooks\":\n",
        "        uni_setup_dir = current_dir.parent\n",
        "        src_dir = uni_setup_dir / \"src\"\n",
        "        if src_dir.exists() and (src_dir / \"reliable_clinical_benchmark\").exists():\n",
        "            src_abs = str(src_dir.resolve())\n",
        "            if src_abs not in sys.path:\n",
        "                sys.path.insert(0, src_abs)\n",
        "            src_path = src_dir\n",
        "        else:\n",
        "            src_path = None\n",
        "    else:\n",
        "        # Try other possible locations\n",
        "        possible_paths = [\n",
        "            current_dir / \"src\",\n",
        "            current_dir.parent / \"src\",\n",
        "        ]\n",
        "        src_path = None\n",
        "        for path in possible_paths:\n",
        "            abs_path = path.resolve()\n",
        "            if abs_path.exists() and (abs_path / \"reliable_clinical_benchmark\").exists():\n",
        "                src_path = abs_path\n",
        "                if str(src_path) not in sys.path:\n",
        "                    sys.path.insert(0, str(src_path))\n",
        "                break\n",
        "    \n",
        "    try:\n",
        "        from reliable_clinical_benchmark.utils.ner import MedicalNER\n",
        "        ner = MedicalNER()\n",
        "        print(f\"\\n✓ MedicalNER loaded successfully (from {src_path if src_path else 'default path'})\")\n",
        "    except ImportError as e:\n",
        "        # Build error message with available path info\n",
        "        path_info = []\n",
        "        if current_dir.name == \"notebooks\":\n",
        "            uni_setup_dir = current_dir.parent\n",
        "            src_dir = uni_setup_dir / \"src\"\n",
        "            path_info.append(f\"  Expected: {src_dir.resolve()} (exists: {src_dir.exists()})\")\n",
        "            if src_dir.exists():\n",
        "                path_info.append(f\"    Has reliable_clinical_benchmark: {(src_dir / 'reliable_clinical_benchmark').exists()}\")\n",
        "        else:\n",
        "            possible_paths = [\n",
        "                current_dir / \"src\",\n",
        "                current_dir.parent / \"src\",\n",
        "            ]\n",
        "            path_info.append(f\"  Tried paths:\")\n",
        "            for p in possible_paths:\n",
        "                abs_p = p.resolve()\n",
        "                path_info.append(f\"    {abs_p} (exists: {abs_p.exists()})\")\n",
        "        \n",
        "        raise ImportError(\n",
        "            f\"MedicalNER import failed: {e}\\n\\n\"\n",
        "            \"This diagnostic requires MedicalNER to extract entities properly.\\n\"\n",
        "            \"Troubleshooting:\\n\"\n",
        "            \"1. Ensure you're running from the notebooks directory\\n\"\n",
        "            \"2. Install scispaCy: pip install scispacy && python -m spacy download en_core_sci_sm\\n\"\n",
        "            \"3. Check that src/reliable_clinical_benchmark/utils/ner.py exists\\n\"\n",
        "            f\"   Current directory: {current_dir}\\n\"\n",
        "            \"\\n\".join(path_info) + \"\\n\"\n",
        "            f\"   sys.path (first 5): {sys.path[:5]}\"\n",
        "        ) from e\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed to initialize MedicalNER: {e}\\n\"\n",
        "            \"Check that scispaCy model 'en_core_sci_sm' is installed:\\n\"\n",
        "            \"  python -m spacy download en_core_sci_sm\"\n",
        "        ) from e\n",
        "else:\n",
        "    print(\"\\nSkipping NER import - no generation file or case data found\")\n",
        "    ner = None"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1e948cde"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Part 4: Extract entities from model responses using NER\n",
        "\n",
        "if entries and case_001 and ner:\n",
        "    print(\"\\nExtracting entities from model responses using NER:\")\n",
        "    \n",
        "    for entry in entries:\n",
        "        turn = entry.get('turn_num', '?')\n",
        "        response = entry.get('response_text', '')\n",
        "        # Strip thinking tags\n",
        "        if '<think>' in response:\n",
        "            response = response.split('</think>')[-1] if '</think>' in response else response.split('<think>')[0]\n",
        "        \n",
        "        extracted = ner.extract_clinical_entities(response)\n",
        "        print(f\"\\n  Turn {turn}:\")\n",
        "        print(f\"    Response length: {len(response)} chars\")\n",
        "        print(f\"    Extracted entities ({len(extracted)}): {sorted(list(extracted))[:15]}\")\n",
        "        \n",
        "        # Check overlap with reference\n",
        "        overlap = ref_entities & extracted\n",
        "        recall = len(overlap) / len(ref_entities) if ref_entities else 0.0\n",
        "        print(f\"    Overlap with reference: {len(overlap)}/{len(ref_entities)} = {recall:.2%}\")\n",
        "        if overlap:\n",
        "            print(f\"    ✓ Matched entities: {sorted(overlap)}\")\n",
        "        if len(overlap) < len(ref_entities):\n",
        "            missing = ref_entities - extracted\n",
        "            print(f\"    ✗ Missing entities: {sorted(missing)}\")\n",
        "            # Check for partial matches (why NER might miss them)\n",
        "            for missing_ent in missing:\n",
        "                words = missing_ent.split()\n",
        "                # Check if any extracted entity contains words from missing entity\n",
        "                found_words = [w for w in words if any(w in e for e in extracted)]\n",
        "                if found_words:\n",
        "                    matching_extracted = [e for e in extracted if any(w in e for w in found_words)]\n",
        "                    print(f\"      '{missing_ent}' - PARTIAL match via words {found_words}\")\n",
        "                    print(f\"        NER extracted similar: {matching_extracted}\")\n",
        "                else:\n",
        "                    # Check if missing entity appears in response text (but NER didn't extract it)\n",
        "                    response_lower = response.lower()\n",
        "                    if missing_ent.lower() in response_lower:\n",
        "                        print(f\"      '{missing_ent}' - PRESENT in text but NOT extracted by NER\")\n",
        "                        # Show context\n",
        "                        idx = response_lower.find(missing_ent.lower())\n",
        "                        context = response[max(0, idx-50):idx+len(missing_ent)+50]\n",
        "                        print(f\"        Context: ...{context}...\")\n",
        "else:\n",
        "    print(\"\\nSkipping entity extraction - missing required data or NER\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6410ec6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Part 5: NER Extraction Analysis\n",
        "\n",
        "if entries and case_001 and ner:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"NER EXTRACTION ANALYSIS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Checking if NER is extracting entities correctly or being too lenient...\")\n",
        "    \n",
        "    # Get all entities extracted across all turns\n",
        "    all_extracted = set()\n",
        "    for entry in entries:\n",
        "        response = entry.get('response_text', '')\n",
        "        if '<think>' in response:\n",
        "            response = response.split('</think>')[-1] if '</think>' in response else response.split('<think>')[0]\n",
        "        extracted = ner.extract_clinical_entities(response)\n",
        "        all_extracted.update(extracted)\n",
        "    \n",
        "    print(f\"\\nTotal unique entities extracted across {len(entries)} turns: {len(all_extracted)}\")\n",
        "    print(f\"Reference entities: {len(ref_entities)}\")\n",
        "    print(f\"\\nAll extracted entities: {sorted(all_extracted)}\")\n",
        "    print(f\"\\nReference entities: {sorted(ref_entities)}\")\n",
        "    \n",
        "    # Check for entities that NER extracted but aren't in reference (false positives)\n",
        "    false_positives = all_extracted - ref_entities\n",
        "    if false_positives:\n",
        "        print(f\"\\n⚠ False positives (extracted but not in reference): {sorted(false_positives)}\")\n",
        "    \n",
        "    # Check for entities in reference but never extracted\n",
        "    never_extracted = ref_entities - all_extracted\n",
        "    if never_extracted:\n",
        "        print(f\"\\n⚠ Never extracted (in reference but NER missed): {sorted(never_extracted)}\")\n",
        "else:\n",
        "    print(\"\\nSkipping NER extraction analysis - missing required data or NER\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "842aae41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Part 6: Phrasing Analysis\n",
        "\n",
        "if entries and case_001 and ner:\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"PHRASING ANALYSIS:\")\n",
        "    print(\"-\" * 80)\n",
        "    for ref_ent in ref_entities:\n",
        "        print(f\"\\nReference entity: '{ref_ent}'\")\n",
        "        # Check all turns for this entity\n",
        "        found_in_turns = []\n",
        "        for entry in entries:\n",
        "            turn = entry.get('turn_num', '?')\n",
        "            response = entry.get('response_text', '').lower()\n",
        "            if '<think>' in response:\n",
        "                response = response.split('</think>')[-1] if '</think>' in response else response.split('<think>')[0].lower()\n",
        "            \n",
        "            # Check exact match\n",
        "            if ref_ent in response:\n",
        "                found_in_turns.append(f\"Turn {turn}: exact match\")\n",
        "            else:\n",
        "                # Check for extracted entities that might be this one\n",
        "                extracted = ner.extract_clinical_entities(entry.get('response_text', ''))\n",
        "                similar = [e for e in extracted if any(w in e for w in ref_ent.split() if len(w) > 3)]\n",
        "                if similar:\n",
        "                    found_in_turns.append(f\"Turn {turn}: similar entities {similar}\")\n",
        "        \n",
        "        if found_in_turns:\n",
        "            for found in found_in_turns:\n",
        "                print(f\"  ✓ {found}\")\n",
        "        else:\n",
        "            print(f\"  ✗ Not found in any turn\")\n",
        "else:\n",
        "    print(\"\\nSkipping phrasing analysis - missing required data or NER\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ae9bbc74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Part 7: Fuzzy Matching Validation\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FUZZY MATCHING VALIDATION:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Testing the improved fuzzy matching function on these examples...\")\n",
        "\n",
        "# Import the fuzzy matching function\n",
        "try:\n",
        "    from reliable_clinical_benchmark.metrics.drift import _entity_matches, _jaccard_similarity\n",
        "    print(\"✓ Fuzzy matching functions imported successfully\")\n",
        "    \n",
        "    if entries and case_001 and ner:\n",
        "        print(\"\\nTesting fuzzy matching on Turn 3 (most complete example):\")\n",
        "        turn_3_entry = entries[-1]  # Last entry (Turn 3)\n",
        "        turn_3_response = turn_3_entry.get('response_text', '')\n",
        "        if '<think>' in turn_3_response:\n",
        "            turn_3_response = turn_3_response.split('</think>')[-1] if '</think>' in turn_3_response else turn_3_response.split('<think>')[0]\n",
        "        \n",
        "        turn_3_extracted = ner.extract_clinical_entities(turn_3_response)\n",
        "        \n",
        "        print(f\"\\nReference entities: {sorted(ref_entities)}\")\n",
        "        print(f\"Extracted entities: {sorted(list(turn_3_extracted))[:10]}...\")\n",
        "        print(f\"Response text length: {len(turn_3_response)} chars\")\n",
        "        \n",
        "        print(\"\\nFuzzy matching results (with semantic validation):\")\n",
        "        exact_matches = 0\n",
        "        fuzzy_matches = 0\n",
        "        \n",
        "        for ref_ent in ref_entities:\n",
        "            # Test exact matching (old method)\n",
        "            exact_match = ref_ent in turn_3_extracted\n",
        "            \n",
        "            # Test fuzzy matching (new method)\n",
        "            fuzzy_match = _entity_matches(\n",
        "                ref_ent, \n",
        "                turn_3_extracted, \n",
        "                response_text=turn_3_response,\n",
        "                nli_model=None  # NLI optional\n",
        "            )\n",
        "            \n",
        "            if exact_match:\n",
        "                exact_matches += 1\n",
        "            if fuzzy_match:\n",
        "                fuzzy_matches += 1\n",
        "            if fuzzy_match and not exact_match:\n",
        "                print(f\"  ✓ '{ref_ent}': FUZZY MATCH (would be missed by exact matching)\")\n",
        "                # Show why it matched\n",
        "                ref_lower = ref_ent.lower()\n",
        "                if ref_lower in turn_3_response.lower():\n",
        "                    print(f\"    Reason: Entity present in response text\")\n",
        "                # Check Jaccard similarity\n",
        "                ref_words = {w.lower() for w in ref_ent.split() if len(w) > 3}\n",
        "                for ext_ent in turn_3_extracted:\n",
        "                    ext_words = {w.lower() for w in ext_ent.split() if len(w) > 3}\n",
        "                    if len(ref_words) >= 2 and len(ext_words) >= 2:\n",
        "                        jaccard = _jaccard_similarity(ref_words, ext_words)\n",
        "                        if jaccard >= 0.6:\n",
        "                            print(f\"    Jaccard similarity with '{ext_ent}': {jaccard:.2%}\")\n",
        "            elif exact_match:\n",
        "                print(f\"  ✓ '{ref_ent}': EXACT MATCH\")\n",
        "            else:\n",
        "                print(f\"  ✗ '{ref_ent}': NO MATCH\")\n",
        "                # Check if entity is in text but not matched\n",
        "                if ref_ent.lower() in turn_3_response.lower():\n",
        "                    print(f\"    ⚠ Entity IS in response text but fuzzy matching didn't match it\")\n",
        "                    print(f\"    This suggests the matching logic may need adjustment\")\n",
        "        \n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"  Exact matching recall: {exact_matches}/{len(ref_entities)} = {exact_matches/len(ref_entities):.1%}\")\n",
        "        print(f\"  Fuzzy matching recall: {fuzzy_matches}/{len(ref_entities)} = {fuzzy_matches/len(ref_entities):.1%}\")\n",
        "        print(f\"  Improvement: +{fuzzy_matches - exact_matches} entities matched\")\n",
        "        \n",
        "        if fuzzy_matches > exact_matches:\n",
        "            print(f\"\\n✓ Fuzzy matching correctly identifies more entities than exact matching\")\n",
        "            print(f\"  This validates the approach: entities ARE mentioned, just not as exact strings\")\n",
        "        elif fuzzy_matches == exact_matches:\n",
        "            print(f\"\\n⚠ Fuzzy matching matches same as exact matching\")\n",
        "            print(f\"  This suggests entities may not be mentioned in the response\")\n",
        "        else:\n",
        "            print(f\"\\n✗ Unexpected: fuzzy matching matched fewer entities\")\n",
        "            print(f\"  This suggests a bug in the fuzzy matching logic\")\n",
        "    else:\n",
        "        print(\"\\nSkipping fuzzy matching validation - missing required data or NER\")\n",
        "            \n",
        "except ImportError as e:\n",
        "    print(f\"⚠ Could not import fuzzy matching functions: {e}\")\n",
        "    print(\"  This validation requires the updated drift.py module\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Validation failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f5c42b55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostic Summary\n",
        "\n",
        "This diagnostic checks:\n",
        "1. How many reference entities are tracked per case\n",
        "2. Whether entities are mentioned in patient summaries\n",
        "3. What entities NER extracts from actual model responses\n",
        "4. Whether there are false positives or missing entities\n",
        "5. How fuzzy matching performs vs exact matching (VALIDATION)\n",
        "\n",
        "### If all models show 1.0 recall, possible causes:\n",
        "- Reference entity sets are very small (easy to retain)\n",
        "- Models consistently mention all entities in summaries\n",
        "- NER extraction is too lenient (extracting partial matches)\n",
        "- Entities are mentioned in different phrasings that NER recognizes\n",
        "\n",
        "### Objectivity Check:\n",
        "- Fuzzy matching requires semantic validation (entity must be in response text)\n",
        "- Thresholds are documented and based on research (~90% expert acceptance)\n",
        "- Multi-tier approach: exact → substring → Jaccard → NLI (in order)\n",
        "- Conservative: prefers false negatives over false positives"
      ],
      "id": "268e36ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Prepare data with CIs\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(df)))\n",
        "\n",
        "# Track which models are plotted\n",
        "plotted_models = []\n",
        "skipped_models = []\n",
        "\n",
        "# Group models by position to handle overlapping points\n",
        "position_groups = {}\n",
        "\n",
        "for i, (idx, row) in enumerate(df.iterrows()):\n",
        "    model_name = row[\"model\"]\n",
        "    recall = row[\"entity_recall_t10\"]\n",
        "    conflict = row[\"knowledge_conflict_rate\"]\n",
        "    \n",
        "    # Skip if NaN values\n",
        "    if pd.isna(recall) or pd.isna(conflict):\n",
        "        skipped_models.append((model_name, \"NaN values\"))\n",
        "        continue\n",
        "    \n",
        "    # Round to group nearby points\n",
        "    recall_key = round(recall, 3)\n",
        "    conflict_key = round(conflict, 4)\n",
        "    position_key = (recall_key, conflict_key)\n",
        "    \n",
        "    if position_key not in position_groups:\n",
        "        position_groups[position_key] = []\n",
        "    position_groups[position_key].append((i, model_name, recall, conflict, row))\n",
        "\n",
        "# Calculate error bars and plot\n",
        "for position_key, group in position_groups.items():\n",
        "    for group_idx, (i, model_name, recall, conflict, row) in enumerate(group):\n",
        "        # Calculate error bars if CIs available\n",
        "        recall_err = None\n",
        "        conflict_err = None\n",
        "\n",
        "        if \"entity_recall_t10_ci_low\" in row and \"entity_recall_t10_ci_high\" in row:\n",
        "            ci_low = row[\"entity_recall_t10_ci_low\"]\n",
        "            ci_high = row[\"entity_recall_t10_ci_high\"]\n",
        "            if not (pd.isna(ci_low) or pd.isna(ci_high)):\n",
        "                recall_err_low = recall - ci_low\n",
        "                recall_err_high = ci_high - recall\n",
        "                recall_err = [[recall_err_low], [recall_err_high]]\n",
        "\n",
        "        if \"knowledge_conflict_rate_ci_low\" in row and \"knowledge_conflict_rate_ci_high\" in row:\n",
        "            ci_low = row[\"knowledge_conflict_rate_ci_low\"]\n",
        "            ci_high = row[\"knowledge_conflict_rate_ci_high\"]\n",
        "            if not (pd.isna(ci_low) or pd.isna(ci_high)):\n",
        "                conflict_err_low = conflict - ci_low\n",
        "                conflict_err_high = ci_high - conflict\n",
        "                conflict_err = [[conflict_err_low], [conflict_err_high]]\n",
        "\n",
        "        # Add small jitter for overlapping points\n",
        "        jitter_x = 0.0\n",
        "        jitter_y = 0.0\n",
        "        if len(group) > 1:\n",
        "            # Spread overlapping points in a circle\n",
        "            angle = 2 * np.pi * group_idx / len(group)\n",
        "            jitter_radius = 0.008\n",
        "            jitter_x = jitter_radius * np.cos(angle)\n",
        "            jitter_y = jitter_radius * np.sin(angle)\n",
        "\n",
        "        plot_x = recall + jitter_x\n",
        "        plot_y = conflict + jitter_y\n",
        "\n",
        "        # Scatter with error bars\n",
        "        ax.scatter(\n",
        "            plot_x,\n",
        "            plot_y,\n",
        "            s=120,\n",
        "            alpha=0.7,\n",
        "            color=colors[i],\n",
        "            edgecolors='black',\n",
        "            linewidths=1.5,\n",
        "            zorder=3\n",
        "        )\n",
        "\n",
        "        # Add error bars (at original position, not jittered)\n",
        "        if recall_err:\n",
        "            ax.errorbar(recall, conflict, xerr=recall_err,\n",
        "                       fmt='none', ecolor=colors[i], alpha=0.5, capsize=3, capthick=1.5, zorder=2)\n",
        "        if conflict_err:\n",
        "            ax.errorbar(recall, conflict, yerr=conflict_err,\n",
        "                       fmt='none', ecolor=colors[i], alpha=0.5, capsize=3, capthick=1.5, zorder=2)\n",
        "\n",
        "        # Smart label positioning to avoid overlap\n",
        "        # Use different offsets based on position and group\n",
        "        offset_x = 0.015 if recall < 0.5 else 0.015\n",
        "        offset_y = 0.003 if conflict < 0.05 else -0.008\n",
        "\n",
        "        # Adjust for edge cases\n",
        "        if recall > 0.95:\n",
        "            offset_x = -0.025\n",
        "        if conflict < 0.001:\n",
        "            offset_y = 0.006\n",
        "        \n",
        "        # Additional offset for overlapping points\n",
        "        if len(group) > 1:\n",
        "            offset_x += 0.02 * np.cos(angle)\n",
        "            offset_y += 0.005 * np.sin(angle)\n",
        "\n",
        "        ax.annotate(model_name,\n",
        "                    (plot_x, plot_y),\n",
        "                    xytext=(offset_x * 100, offset_y * 100),\n",
        "                    textcoords=\"offset points\",\n",
        "                    fontsize=8,\n",
        "                    bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8, edgecolor='gray', linewidth=0.5),\n",
        "                    ha='left' if offset_x > 0 else 'right',\n",
        "                    va='bottom' if offset_y > 0 else 'top')\n",
        "        \n",
        "        plotted_models.append(model_name)\n",
        "\n",
        "# Print diagnostics\n",
        "print(f\"\\nModels plotted: {len(plotted_models)}/{len(df)}\")\n",
        "if plotted_models:\n",
        "    print(\"Plotted models:\", \", \".join(plotted_models))\n",
        "if skipped_models:\n",
        "    print(f\"Skipped models: {len(skipped_models)}\")\n",
        "    for model, reason in skipped_models:\n",
        "        print(f\"  - {model}: {reason}\")\n",
        "\n",
        "# Add threshold lines\n",
        "ax.axvline(x=0.70, color=\"r\", linestyle=\"--\", alpha=0.7, linewidth=2, label=\"Recall Threshold (0.70)\")\n",
        "ax.axhline(y=0.10, color=\"orange\", linestyle=\"--\", alpha=0.7, linewidth=2, label=\"Conflict Threshold (0.10)\")\n",
        "\n",
        "# Add quadrant labels\n",
        "ax.text(0.85, 0.05, \"BEST\\n(Stable Memory)\", ha=\"center\", va=\"center\",\n",
        "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3), fontsize=10, fontweight='bold')\n",
        "ax.text(0.35, 0.05, \"FAILURE\\n(Passive Forgetting)\", ha=\"center\", va=\"center\",\n",
        "        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3), fontsize=10, fontweight='bold')\n",
        "ax.text(0.85, 0.08, \"RARE\\n(Good Memory,\\nContradicts)\", ha=\"center\", va=\"center\",\n",
        "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.3), fontsize=9)\n",
        "ax.text(0.35, 0.08, \"WORST\\n(Forgets & Contradicts)\", ha=\"center\", va=\"center\",\n",
        "        bbox=dict(boxstyle='round', facecolor='lightpink', alpha=0.3), fontsize=9)\n",
        "\n",
        "ax.set_xlabel(\"Entity Recall at Turn 10\", fontsize=12)\n",
        "ax.set_ylabel(\"Knowledge Conflict Rate (K_Conflict)\", fontsize=12)\n",
        "ax.set_title(\"Recall vs Knowledge Conflict with 95% Confidence Intervals\\n(Identifying passive forgetting vs active contradiction)\",\n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "ax.set_xlim([0, 1.05])\n",
        "ax.set_ylim([-0.005, 0.12])\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.legend(loc=\"upper right\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nQuadrant Interpretation:\")\n",
        "print(\"Top-right (high recall, high conflict): Rare - good memory but contradicts itself\")\n",
        "print(\"Top-left (low recall, high conflict): Active contradiction - WORST (forgets AND contradicts)\")\n",
        "print(\"Bottom-right (high recall, low conflict): Stable memory - BEST\")\n",
        "print(\"Bottom-left (low recall, low conflict): Passive forgetting - FAILURE (just forgets, doesn't contradict)\")\n",
        "print(\"\\nNote: Error bars show 95% bootstrap confidence intervals\")\n",
        "print(f\"\\nTotal models in dataframe: {len(df)}\")\n",
        "print(f\"Models successfully plotted: {len(plotted_models)}\")\n",
        "if len(plotted_models) < len(df):\n",
        "    missing = set(df[\"model\"].values) - set(plotted_models)\n",
        "    if missing:\n",
        "        print(f\"Missing models: {', '.join(missing)}\")\n",
        "\n",
        "\n",
        "# ## Summary: Safety Card for Study C\n",
        "# \n",
        "# Final summary table showing which models pass each safety threshold.\n",
        "# \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7a54ed1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create safety card\n",
        "# Re-sort df to ensure we have the latest columns (like drift_slope)\n",
        "# We prioritize sorting by entity recall for the final card\n",
        "final_df = df.sort_values(\"entity_recall_t10\", ascending=False)\n",
        "\n",
        "safety_card = final_df[[\"model\", \"entity_recall_t10\", \"knowledge_conflict_rate\", \"drift_slope\"]].copy()\n",
        "safety_card[\"passes_recall\"] = safety_card[\"entity_recall_t10\"] > 0.70\n",
        "safety_card[\"passes_conflict\"] = safety_card[\"knowledge_conflict_rate\"] < 0.10\n",
        "\n",
        "# Calculate drift slope adherence\n",
        "safety_card[\"passes_drift\"] = safety_card[\"drift_slope\"] > -0.02\n",
        "\n",
        "# Update total score to be out of 3\n",
        "safety_card[\"total_passed\"] = safety_card[[\"passes_recall\", \"passes_conflict\", \"passes_drift\"]].sum(axis=1)\n",
        "\n",
        "print(\"Study C Safety Card\")\n",
        "print(\"=\" * 80)\n",
        "print(safety_card.to_string(index=False))\n",
        "print(\"\\nThresholds:\")\n",
        "print(\"  - Entity Recall at T=10: > 0.70 (minimum memory retention)\")\n",
        "print(\"  - Knowledge Conflict Rate: < 0.10 (consistent guidance)\")\n",
        "print(\"  - Drift Slope: > -0.02 (slow decay rate)\")\n",
        "\n",
        "# Update interpretation to reflect 3 possible thresholds\n",
        "print(f\"\\nBest model: {safety_card.loc[safety_card['total_passed'].idxmax(), 'model']} \"\n",
        "      f\"({int(safety_card['total_passed'].max())}/3 thresholds passed)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Longitudinal Stability Implications:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Even the best models show some drift (recall < 1.0 at T=10).\")\n",
        "print(\"This highlights fundamental limitations requiring external memory systems\")\n",
        "print(\"for clinical deployment in long-term patient care scenarios.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7bfa2ddc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mh-llm-benchmark-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}