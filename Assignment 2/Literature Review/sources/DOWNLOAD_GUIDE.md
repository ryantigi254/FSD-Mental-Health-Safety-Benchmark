# Paper Download Guide

This guide lists all 60 papers from the priority reading list, organized by bucket and tier, with direct download links.

## How to Use This Guide

1. **For Tier 1 papers**: Download all papers in `tier_1_must_read` folders first
2. **Click the link** to access the paper (arXiv, conference proceedings, etc.)
3. **Save the PDF** to the corresponding folder using the naming convention: `{Author_Last_Name}_{Year}_{Short_Title}.pdf`
4. **Update PRIORITY_READING_LIST.csv** after downloading (set Reading_Status to "Downloaded")

## Bucket A: Faithfulness Gap

### tier_1_must_read/ (4 papers)

1. **Lanham et al. (2023)** - "Measuring Faithfulness in Chain-of-Thought Reasoning"
   - Link: https://arxiv.org/abs/2307.13702
   - Save as: `Lanham_2023_Measuring_Faithfulness.pdf`
   - Folder: `bucket_a_faithfulness/tier_1_must_read/`

2. **Lanham et al. / Paul et al. (2024)** - "Making Reasoning Matter: Measuring and Improving Faithfulness"
   - **Note**: Same paper, different author listings (Lanham and Paul are co-authors)
   - Link: Findings of EMNLP 2024 (search for paper)
   - Save as: `Lanham_2024_Making_Reasoning_Matter.pdf` (or `Paul_2024_Making_Reasoning_Matter.pdf` if Paul is listed first)
   - Folder: `bucket_a_faithfulness/tier_1_must_read/`

3. **Turpin et al. (2023)** - "Language Models Don't Always Say What They Think"
   - Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/ed3fea9033a80fea1376299fa7863f4a-Paper-Conference.pdf
   - Save as: `Turpin_2023_Language_Models_Dont_Always_Say.pdf`
   - Folder: `bucket_a_faithfulness/tier_1_must_read/`

### tier_2_important/ (4 papers)

5. **Lyu et al. (2023)** - "Faithful Chain-of-Thought Reasoning"
   - Link: https://arxiv.org/abs/2301.13379
   - Save as: `Lyu_2023_Faithful_Chain_of_Thought.pdf`
   - Folder: `bucket_a_faithfulness/tier_2_important/`

6. **Paul & West (2024)** - "On Measuring Faithfulness or Self-Consistency"
   - Link: ACL 2024 (search for paper)
   - Save as: `Paul_West_2024_On_Measuring_Faithfulness.pdf`
   - Folder: `bucket_a_faithfulness/tier_2_important/`

7. **Ibrahim et al. (2025)** - "A Causal Lens for Evaluating Faithfulness Metrics"
   - Link: https://arxiv.org/abs/2502.18848
   - Save as: `Ibrahim_2025_Causal_Lens_Faithfulness.pdf`
   - Folder: `bucket_a_faithfulness/tier_2_important/`

8. **FaithCoT-Bench (2025)** - "Benchmarking Instance-Level Faithfulness"
   - Link: https://arxiv.org/html/2510.04040v1
   - Save as: `FaithCoT_Bench_2025_Benchmarking_Faithfulness.pdf`
   - Folder: `bucket_a_faithfulness/tier_2_important/`

### tier_3_reference/ (3 papers)

9. **Tsai et al. (2024)** - "TokenSHAP: Interpreting Large Language Models"
   - Link: https://arxiv.org/abs/2407.10114
   - Save as: `Tsai_2024_TokenSHAP.pdf`
   - Folder: `bucket_a_faithfulness/tier_3_reference/`

10. **Lundberg et al. (2024)** - "Faithful Group Shapley Value"
    - Link: https://openreview.net (search for paper)
    - Save as: `Lundberg_2024_Faithful_Group_Shapley.pdf`
    - Folder: `bucket_a_faithfulness/tier_3_reference/`

11. **ResearchGate (2024)** - "The Probabilities Also Matter"
    - Link: https://www.researchgate.net/publication/384212923
    - Save as: `ResearchGate_2024_Probabilities_Also_Matter.pdf`
    - Folder: `bucket_a_faithfulness/tier_3_reference/`

## Bucket B: Sycophancy

### tier_1_must_read/ (7 papers)

12. **Wei et al. (2023)** - "Simple Synthetic Data Reduces Sycophancy in Large Language Models"
    - Link: https://arxiv.org/abs/2308.03958
    - Save as: `Wei_2023_Simple_Synthetic_Data_Reduces_Sycophancy.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

13. **Anthropic (2024)** - "Towards Understanding Sycophancy in Language Models"
    - Link: https://arxiv.org/abs/2310.13548
    - Save as: `Anthropic_2024_Towards_Understanding_Sycophancy.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

14. **Fanous et al. (2025)** - "SycEval: Evaluating LLM Sycophancy"
    - Link: https://arxiv.org/abs/2502.08177
    - Save as: `Fanous_2025_SycEval.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

15. **Kaur (2025)** - "Echoes of Agreement: Argument-Driven Sycophancy"
    - Link: Findings of EMNLP 2025 (search for paper)
    - Save as: `Kaur_2025_Echoes_of_Agreement.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

16. **Pandey et al. (2025)** - "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy"
    - Link: https://arxiv.org/abs/2510.16727
    - Save as: `Pandey_2025_Beacon.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

17. **Hong et al. (2025)** - "ELEPHANT and SYCON-Bench: Measuring Social Sycophancy"
    - Link: Findings of EMNLP 2025 (search for paper)
    - Save as: `Hong_2025_ELEPHANT_SYCON_Bench.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

18. **Liu et al. (2025)** - "Truth Decay: Quantifying Multi-Turn Sycophancy"
    - Link: https://arxiv.org/abs/2503.11656
    - Save as: `Liu_2025_Truth_Decay.pdf`
    - Folder: `bucket_b_sycophancy/tier_1_must_read/`

### tier_2_important/ (6 papers)

19. **Anthropic (2024)** - "Alignment Faking in Large Language Models"
    - Link: Anthropic Whitepaper (search for paper)
    - Save as: `Anthropic_2024_Alignment_Faking.pdf`
    - Folder: `bucket_b_sycophancy/tier_2_important/`

20. **Koorndijk (2025)** - "Empirical Evidence for Alignment Faking in a Small LLM"
    - Link: https://arxiv.org/abs/2506.21584
    - Save as: `Koorndijk_2025_Alignment_Faking_Small_LLM.pdf`
    - Folder: `bucket_b_sycophancy/tier_2_important/`

21. **Meinke et al. (2024)** - "Frontier Models are Capable of In-Context Scheming"
    - Link: Apollo Research Technical Report (search for paper)
    - Save as: `Meinke_2024_In_Context_Scheming.pdf`
    - Folder: `bucket_b_sycophancy/tier_2_important/`

22. **Van der Weij et al. (2025)** - "AI Sandbagging: Language Models Can Strategically Underperform"
    - Link: ICLR 2025 (search for paper)
    - Save as: `Van_der_Weij_2025_AI_Sandbagging.pdf`
    - Folder: `bucket_b_sycophancy/tier_2_important/`

23. **Zou et al. (2024)** - "Enhancing Multiple Dimensions of Trustworthiness via Sparse Activation Control"
    - Link: NeurIPS 2024 (search for paper)
    - Save as: `Zou_2024_Trustworthiness_Sparse_Activation.pdf`
    - Folder: `bucket_b_sycophancy/tier_2_important/`

24. **Lee et al. (2025)** - "When Helpfulness Backfires: LLMs and the Risk of False Medical Information"
    - Link: Journal of Medical Internet Research (search for paper)
    - Save as: `Lee_2025_Helpfulness_Backfires.pdf`
    - Folder: `bucket_b_sycophancy/tier_2_important/`

### tier_3_reference/ (6 papers)

25. **Google Research (2024)** - "Sycophancy Intervention Repository"
    - Link: https://github.com/google/sycophancy-intervention
    - Save as: `Google_2024_Sycophancy_Intervention_README.pdf` (documentation)
    - Folder: `bucket_b_sycophancy/tier_3_reference/`

26. **Thenraj (2023)** - "E18: Simple Synthetic Data Reduces Sycophancy in LLMs"
    - Link: Medium article (search for article)
    - Save as: `Thenraj_2023_E18_Medium.pdf`
    - Folder: `bucket_b_sycophancy/tier_3_reference/`

27. **Holter (2025)** - "Understanding and Mitigating Sycophancy in AI Models"
    - Link: Technical report (search for paper)
    - Save as: `Holter_2025_Understanding_Sycophancy.pdf`
    - Folder: `bucket_b_sycophancy/tier_3_reference/`

28. **Pi et al. (2025)** - "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal LLMs"
    - Link: Manuscript under review (search for preprint)
    - Save as: `Pi_2025_Multimodal_Sycophancy.pdf`
    - Folder: `bucket_b_sycophancy/tier_3_reference/`

29. **Gretz et al. (2020)** - "A Large-Scale Dataset for Argument Quality Ranking"
    - Link: AAAI 2020 (search for paper)
    - Save as: `Gretz_2020_Argument_Quality_Dataset.pdf`
    - Folder: `bucket_b_sycophancy/tier_3_reference/`

## Bucket C: Silent Bias

### tier_2_important/ (2 papers)

30. **Gabriel et al. (2024)** - "Can AI Relate: Testing Large Language Model Response for Mental Health Support"
    - Link: Findings of EMNLP 2024 (search for paper)
    - Save as: `Gabriel_2024_Can_AI_Relate.pdf`
    - Folder: `bucket_c_silent_bias/tier_2_important/`

31. **Lee et al. (2024)** - "SafeHear: A Novel Evaluation Benchmark for Medical LLMs"
    - Link: https://arxiv.org/abs/2507.23486
    - Save as: `Lee_2024_SafeHear.pdf`
    - Folder: `bucket_c_silent_bias/tier_2_important/`

## Bucket D: Longitudinal Drift

### tier_1_must_read/ (3 papers)

32. **Laban et al. (2025)** - "LLMs Get Lost In Multi-Turn Conversation"
    - Link: https://arxiv.org/pdf/2505.06120
    - Save as: `Laban_2025_LLMs_Get_Lost.pdf`
    - Folder: `bucket_d_longitudinal/tier_1_must_read/`

33. **Kruse et al. (2025)** - "Large Language Models with Temporal Reasoning for Longitudinal Clinical Summarisation"
    - Link: Findings of EMNLP 2025 (search for paper)
    - Save as: `Kruse_2025_Temporal_Reasoning_Clinical.pdf`
    - Folder: `bucket_d_longitudinal/tier_1_must_read/`

34. **Zheng et al. (2024)** - "Why LLMs Fail in Multi-Turn Conversations (And How to Fix It)"
    - Link: https://www.prompthub.us/blog/why-llms-fail-in-multi-turn-conversations-and-how-to-fix-it
    - Save as: `Zheng_2024_Why_LLMs_Fail_Multi_Turn.pdf`
    - Folder: `bucket_d_longitudinal/tier_1_must_read/`

### tier_2_important/ (2 papers)

35. **Yuan et al. (2024)** - "Drift No More? Context Equilibria in Multi-Turn LLM Interactions"
    - Link: https://arxiv.org/abs/2510.07777
    - Save as: `Yuan_2024_Drift_No_More.pdf`
    - Folder: `bucket_d_longitudinal/tier_2_important/`

36. **Context Length Research (2025)** - "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval"
    - Link: https://arxiv.org/abs/2510.05381
    - Save as: `Context_Length_2025_Performance_Hurt.pdf`
    - Folder: `bucket_d_longitudinal/tier_2_important/`

### tier_3_reference/ (2 papers)

37. **Memory Drift Research Group (2024)** - "Can an LLM Induce a Graph? Investigating Memory Drift"
    - Link: Technical report (search for paper)
    - Save as: `Memory_Drift_2024_Graph_Induction.pdf`
    - Folder: `bucket_d_longitudinal/tier_3_reference/`

38. **Orq.ai (2025)** - "Understanding Model Drift and Data Drift in LLMs"
    - Link: Technical blog (search for article)
    - Save as: `Orq_2025_Model_Data_Drift.pdf`
    - Folder: `bucket_d_longitudinal/tier_3_reference/`

## Clinical Domain

### tier_1_must_read/ (3 papers)

39. **Zhang et al. (2025) / PsyLLM** - "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning"
    - Link: https://arxiv.org/abs/2505.15715
    - Save as: `Zhang_2025_Beyond_Empathy_PsyLLM.pdf`
    - Folder: `clinical_domain/tier_1_must_read/`

40. **Hager et al. (2024)** - "Evaluation and Mitigation of the Limitations of Large Language Models in Clinical Decision-Making"
    - Link: Nature Medicine 2024 (search for paper)
    - Save as: `Hager_2024_Clinical_Decision_Making.pdf`
    - Folder: `clinical_domain/tier_1_must_read/`

41. **Lee et al. (2024)** - "SafeHear: A Novel Evaluation Benchmark for Medical LLMs"
    - Link: https://arxiv.org/abs/2507.23486
    - Save as: `Lee_2024_SafeHear_Clinical.pdf`
    - Folder: `clinical_domain/tier_1_must_read/`

### tier_2_important/ (4 papers)

42. **Singhal et al. (2023)** - "Towards Expert-Level Medical Question Answering with Large Language Models"
    - Link: https://arxiv.org/abs/2305.09617
    - Save as: `Singhal_2023_Expert_Level_Medical_QA.pdf`
    - Folder: `clinical_domain/tier_2_important/`

43. **Gabriel et al. (2024)** - "Can AI Relate: Testing Large Language Model Response for Mental Health Support"
    - Link: Findings of EMNLP 2024 (search for paper)
    - Save as: `Gabriel_2024_Can_AI_Relate_Clinical.pdf`
    - Folder: `clinical_domain/tier_2_important/`

44. **Kim et al. (2025)** - "CARE-AD: A Multi-Agent Large Language Model Framework for Alzheimer's Disease Prediction"
    - Link: npj Digital Medicine 2025 (search for paper)
    - Save as: `Kim_2025_CARE_AD.pdf`
    - Folder: `clinical_domain/tier_2_important/`

45. **Chen et al. (2025)** - "DENSE: Longitudinal Progress Note Generation with Temporal Modelling"
    - Link: https://arxiv.org/abs/2507.14079
    - Save as: `Chen_2025_DENSE.pdf`
    - Folder: `clinical_domain/tier_2_important/`

## Evaluation Tools

### tier_1_must_read/ (1 paper)

46. **Kim et al. (2025)** - "Development and Validation of the Provider Documentation Summarisation Quality Instrument"
    - Link: https://arxiv.org/abs/2501.08977
    - Save as: `Kim_2025_PDSQI_9.pdf`
    - Folder: `evaluation_tools/tier_1_must_read/`

### tier_2_important/ (2 papers)

47. **Smith et al. (2025)** - "Automating Evaluation of AI Text Generation in Healthcare with an LLM-as-a-Judge"
    - Link: https://www.medrxiv.org/content/10.1101/2025.04.22.25326219v1
    - Save as: `Smith_2025_LLM_as_Judge.pdf`
    - Folder: `evaluation_tools/tier_2_important/`

48. **Cheng et al. (2025)** - "Evaluating Clinical AI Summaries with LLMs-as-Judges"
    - Link: medRxiv preprint (search for paper)
    - Save as: `Cheng_2025_Clinical_AI_Summaries.pdf`
    - Folder: `evaluation_tools/tier_2_important/`

### tier_3_reference/ (1 paper)

49. **Ragas (2025)** - "Faithfulness Metric Documentation"
    - Link: https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness
    - Save as: `Ragas_2025_Faithfulness_Documentation.pdf` (web page print)
    - Folder: `evaluation_tools/tier_3_reference/`

## Additional Papers (Methods, Surveys, Resources)

### Methods (tier_3_reference/)

50. **Madaan et al. (2023)** - "Self-Refine: Iterative Refinement with Feedback from Large Language Models"
    - Link: https://arxiv.org/abs/2303.17651
    - Save as: `Madaan_2023_Self_Refine.pdf`
    - Folder: `bucket_a_faithfulness/tier_3_reference/` (related to faithfulness)

51. **Shinn et al. (2023)** - "Reflexion: Language Agents with Verbal Reinforcement Learning"
    - Link: https://arxiv.org/abs/2303.11366
    - Save as: `Shinn_2023_Reflexion.pdf`
    - Folder: `bucket_a_faithfulness/tier_3_reference/` (related to faithfulness)

### Surveys (tier_2_important/ and tier_3_reference/)

52. **Zhao et al. (2025)** - "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models"
    - Link: ResearchGate (search for paper)
    - Save as: `Zhao_2025_LLLMs_Survey.pdf`
    - Folder: `bucket_a_faithfulness/tier_2_important/` (general survey)

53. **LessWrong (2023)** - "Measuring and Improving the Faithfulness of Model-Generated Reasoning"
    - Link: LessWrong technical note (search for article)
    - Save as: `LessWrong_2023_Faithfulness.pdf`
    - Folder: `bucket_a_faithfulness/tier_3_reference/`

### Resources (tier_3_reference/)

54. **BIG-Bench Hard (2023)** - Individual task performance data
    - Link: https://www.researchgate.net/figure/BIG-Bench-Hard-1827-individual-task-performance_tbl3_372989811
    - Save as: `BIG_Bench_Hard_2023_Performance_Data.pdf` (figure/table)
    - Folder: `bucket_a_faithfulness/tier_3_reference/`

55. **Jerry Wei's Research (2023)** - Dartmouth College contributions
    - Link: https://www.researchgate.net/scientific-contributions/Jerry-Wei-2152948001
    - Save as: `Jerry_Wei_2023_Research_Profile.pdf` (profile page)
    - Folder: `bucket_b_sycophancy/tier_3_reference/`

56. **Evidently AI (2024)** - "10 LLM Safety and Bias Benchmarks"
    - Link: https://www.evidentlyai.com/blog/llm-safety-bias-benchmarks
    - Save as: `Evidently_AI_2024_Safety_Bias_Benchmarks.pdf` (blog article)
    - Folder: `bucket_c_silent_bias/tier_3_reference/`

57. **BenchRisk (2024)** - "Risk Management for Mitigating Benchmark Failure Modes"
    - Link: https://openreview.net/pdf?id=YAGa8upUSA
    - Save as: `BenchRisk_2024_Risk_Management.pdf`
    - Folder: `bucket_a_faithfulness/tier_3_reference/`

58. **Hugging Face (2024)** - Papers: Mental Health Counseling
    - Link: https://huggingface.co/papers?q=mental%20health%20counseling
    - Save as: `HuggingFace_2024_Mental_Health_Papers.pdf` (search results)
    - Folder: `clinical_domain/tier_3_reference/`

## Download Tips

1. **arXiv papers**: Click the PDF link on the arXiv page
2. **Conference papers**: Search for the paper on the conference website or use Google Scholar
3. **Journal papers**: Access through your institution's library or use the DOI
4. **Technical reports**: Search for the paper title + author name
5. **GitHub repositories**: Download README or documentation as PDF

## Batch Download Script (Optional)

For arXiv papers, you can use this PowerShell script to download multiple papers:

```powershell
# Example: Download arXiv papers
$papers = @(
    "2307.13702",  # Lanham 2023
    "2308.03958",  # Wei 2023
    "2505.06120"   # Laban 2025
)

foreach ($paper in $papers) {
    $url = "https://arxiv.org/pdf/$paper.pdf"
    $output = "paper_$paper.pdf"
    Invoke-WebRequest -Uri $url -OutFile $output
}
```

## Notes

- Some papers may require institutional access (Nature Medicine, conference proceedings)
- For papers marked "search for paper", use Google Scholar or the conference/journal website
- Update `PRIORITY_READING_LIST.csv` after downloading each paper
- If a link is broken, try searching for the paper title on Google Scholar

