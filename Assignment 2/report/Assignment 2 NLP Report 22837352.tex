% !TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

% --- Packages ---
\usepackage[margin=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, 
    linkcolor=blue, 
    urlcolor=blue, 
    citecolor=blue,
    pdftitle={NLP Clinical Safety Evaluation Report},
    pdfauthor={Ryan Mutiga Gichuru}
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}

% --- Settings ---
\setstretch{1.15}
\graphicspath{{images/}} 

% Code listing style
\definecolor{codegray}{rgb}{0.95,0.95,0.95}
\definecolor{codeblue}{rgb}{0.1,0.1,0.6}
\definecolor{codegreen}{rgb}{0.1,0.5,0.1}
\lstset{
    backgroundcolor=\color{codegray},
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue}\bfseries,
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    captionpos=b,
    tabsize=4
}

% --- Metadata ---
\title{\textbf{Reliable Clinical Reasoning in Mental-Health LLMs:}\\
\large A Rigorous Evaluation of Faithfulness, Sycophancy, and Longitudinal Drift}
\author{Ryan Mutiga Gichuru\\
\small CSY3055 Natural Language Processing -- Assignment 2}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive black-box evaluation of reasoning models (8B--32B parameters) intended for mental-health decision support. Moving beyond static medical knowledge benchmarks, we audit "reasoning reliability" through three high-stakes failure modes: (1) \textbf{Faithfulness}, (2) \textbf{Sycophancy}, and (3) \textbf{Longitudinal Drift}. Utilising a harness of over 15,000 prompts, we find that modern reasoning models have effectively solved short-term context drift (\texttt{Recall@T10}: 1.0 across all models). However, a critical trade-off remains in reasoning integrity: while \texttt{Psyche-R1} achieves an excellent faithfulness gap (-0.02), it exhibits a dangerous 71\% silent bias rate. Conversely, \texttt{PsyLLM} demonstrates superior reasoning content (\texttt{Step-F1}: 0.11) but struggles with unfaithful rationalisation. We propose a "Minimum Viable Auditing Harness" for clinical AI deployment.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
The deployment of Large Language Models (LLMs) in healthcare, specifically for mental health triage and support, carries substantial epistemic risk. A model may generate a correct diagnosis for the wrong reasons (unfaithful reasoning), capitulate to a patient's dangerous misconceptions (sycophancy), or forget critical medical history during a long conversation (drift).

This benchmark operationalises these risks into measurable metrics. We evaluate a diverse cohort of models, including domain-specific fine-tunes (\texttt{PsyLLM}, \texttt{Psych-Qwen}, \texttt{Psyche-R1}) and general-purpose reasoners (\texttt{GPT-OSS-20B}, \texttt{QwQ}, \texttt{DeepSeek-R1-14B}), to understand the trade-offs between parameter scale, domain specialisation, and safety.

\section{Literature Review: The Epistemological Crisis in Clinical AI}

The integration of LLMs into mental health care represents a paradigm shift from retrieval-based systems to generative reasoning agents. While general-purpose models have achieved expert-level performance on static medical benchmarks such as MedQA and the USMLE \cite{singhal2023expert}, recent literature suggests these metrics mask profound fragilities in reasoning reliability, particularly when models are deployed in dynamic, multi-turn clinical interactions \cite{he2025survey}. This review examines the three critical failure modes—unfaithful reasoning, sycophancy, and longitudinal drift—that necessitate the rigorous auditing framework proposed in this study.

\subsection{The Faithfulness Gap in Chain-of-Thought Reasoning}
A central premise of clinical AI safety is that a model’s explanation (Chain-of-Thought, CoT) must faithfully reflect the computational process used to derive the diagnosis. However, Turpin et al. (2023) demonstrated that LLMs frequently engage in "rationalisation," generating plausible clinical justifications for incorrect answers simply because the answer was biased by the prompt context \cite{turpin2023lmdont}. Their study on BIG-Bench Hard tasks revealed that CoT explanations can be systematically misleading, serving as post-hoc justifications rather than true causal traces.

This disconnect was formalised by Lanham et al. (2024) through the \textit{Faithfulness Gap} metric \cite{lanham2024reasoning}. Their work revealed that for many 7B--30B parameter models, the CoT is often "decorative"—removing the reasoning step does not alter the prediction, implying the model operates on intuition rather than logic. In the mental health domain, where the \textit{process} of differential diagnosis is as critical as the label, such unfaithfulness presents a severe safety risk (e.g., a correct suicide risk assessment based on spurious correlations).

\subsection{Sycophancy and Truth Decay}
Clinical safety requires an agent to maintain objective medical truth, even when contradicted by a patient. However, Reinforcement Learning from Human Feedback (RLHF) often induces \textit{sycophancy}—the tendency to align with user views to maximise approval reward. Wei et al. (2023) established that even sophisticated models will agree with objectively wrong statements if the user asserts them confidently \cite{wei2023sycophancy}, a behaviour that can be mitigated with simple synthetic data interventions.

In the clinical domain, this failure mode is malignant. Stadia et al. (2024) in their work "Can AI Relate" \cite{stadia2024can} highlighted that models attempting to be empathetic often over-agree with users, potentially reinforcing dangerous delusions or minimising symptoms. Furthermore, recent work on "Truth Decay" identifies that a model's adherence to factual truth can degrade significantly (up to 47\% accuracy drops) as it is repeatedly challenged in multi-turn conversations \cite{liu2025truth}. This suggests that single-turn benchmarks are insufficient for evaluating mental health chatbots that must maintain therapeutic boundaries over time.

\subsection{Domain Specialisation and Alignment Faking}
To mitigate these risks, recent efforts have focused on domain-specific fine-tuning. Hu et al. (2025) introduced \texttt{PsyLLM}, a model fine-tuned on detailed psychological reasoning traces (e.g., CBT, ACT frameworks) \cite{hu2025psyllm}. While \texttt{PsyLLM} often outperforms generalist baselines in generating empathetic responses, it remains unclear whether fine-tuning improves actual reasoning robustness or merely mimics the \textit{style} of clinical empathy without underlying logic.

\section{Clinical Evaluation Framework}
\subsection{Overall Architecture}
Our evaluation harness abstracts the assessment process into three distinct engines, fed by synthetic clinical vignettes and governed by a strict safety policy. 

\subsubsection{Justification of Evaluation Personas}
We utilise clinically distinct personas to ensure valid testing of specific model failures:
\begin{itemize}
    \item \textbf{Persona 'Maya' (EUPD/BPD Traits)}: Maya presents with high emotional volatility and rejection sensitivity. This persona challenges the specific model failure of \textit{Sycophancy} and \textit{Boundary Maintenance}. Models often fail by becoming overly agreeable to de-escalate Maya's distress, inadvertently validating her maladaptive schemas.
    \item \textbf{Persona 'Autistic Teen' (Sensory Overload)}: This persona requires concise, concrete communication and low ambiguity tolerance. It serves as a stress test for \textit{Instruction Following} and \textit{Faithfulness}. Verbose, "decorative" reasoning (hallucinated empathy) is actively harmful here, allowing us to measure if the model can adapt its output style without losing clinical accuracy.
\end{itemize}

\begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{Images/End-to-End Architecture Framework Jan 28 2026.png}
        \caption{\textbf{Evaluation Architecture}. The system ingests clinical vignettes, injects adversarial probes (e.g., opinion pressure), and uses "LLM-as-a-Judge" or deterministic scripts to score outputs.}
    \label{fig:architecture}
\end{figure}

\subsection{Metric Philosophy: Primary vs. Diagnostic}
To avoid "analysis paralysis," we categorise metrics into two tiers:
\begin{itemize}
    \item \textbf{Primary Metrics} (Pass/Fail): The headline number proving a failure exists (e.g., \textit{Sycophancy Probability}). These are crucial for legal and safety liability—a model that agrees with a patient's self-harm plan is an immediate deployment failure.
    \item \textbf{Diagnostic Metrics} (Mechanism): Explains \textit{why} it failed (e.g., \textit{Step-F1} reveals if the reasoning logic was flawed). High Step-F1 with low Faithfulness suggests the model knows the theory but doesn't use it.
    \item \textbf{Supplementary Metrics} (Deep Investigation): Optional advanced measures for specific failure modes (e.g., \textit{Silent Bias Rate} or \textit{Flip Rate}). These provide granular insight into specific safety risks like demographic bias or clinical harm without cluttering the high-level metrics.
\end{itemize}

\section{Study A: Faithfulness Evaluation}
\textbf{Research Question:} \textit{Does the model's "Chain of Thought" (CoT) actually drive its answer?}

\subsection{Methodology}
We measure the \textbf{Faithfulness Gap} ($\Delta_{\text{Reasoning}}$), defined as the difference in accuracy between a CoT run and an "Early Answering" run where reasoning is suppressed.
\begin{equation}
\Delta_{\text{Reasoning}} = \text{Acc}_{\text{CoT}} - \text{Acc}_{\text{Early}}
\end{equation}
A gap near zero suggests the reasoning is "decorative"---the model intuitively knows the answer and generates justification post-hoc. A positive gap implies the reasoning aids the decision. 

To validate the \textit{quality} of the reasoning, we compute \textbf{Step-F1} against expert gold-standard traces. We also measure \textbf{Silent Bias} ($R_{SB}$): the rate at which a model makes a biased decision without explicitly mentioning the bias in its reasoning trace.

\subsubsection{Implementation}
\begin{lstlisting}[language=Python, caption={Algorithm for calculating Faithfulness Gap}]
def calculate_faithfulness_gap(model, vignettes):
    score_cot = 0
    score_early = 0
    
    for v in vignettes:
        # Run 1: Chain-of-Thought enabled
        resp_cot = model.generate(v.prompt, mode="cot")
        if is_correct(resp_cot, v.gold_answer):
            score_cot += 1
            
        # Run 2: Forced immediate answer
        resp_early = model.generate(v.prompt, mode="direct")
        if is_correct(resp_early, v.gold_answer):
            score_early += 1
            
    return (score_cot / len(vignettes)) - (score_early / len(vignettes))
\end{lstlisting}

\subsection{Results}
Table \ref{tab:faithfulness} presents the aggregate performance across faithfulness metrics. \texttt{Psyche-R1} achieved the most desirable faithfulness gap ($\Delta = -0.020$), indicating that its reasoning process is tightly coupled to its predictions. However, this model exhibited a critical safety failure with a \textbf{Silent Bias rate ($R_{SB}$) of 0.714}, meaning it acted on demographic bias 71\% of the time without disclosing it in the reasoning trace.

Conversely, \texttt{PsyLLM} demonstrated the highest qualitative reasoning score (\textbf{Step-F1} = 0.110), significantly outperforming the baseline \texttt{Qwen3} (0.027). However, its faithfulness gap was -0.113, suggesting a "reasoning tax" where CoT distracts the model from the correct intuition.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/output.png}
\caption{\textbf{Faithfulness Gap vs. Reasoning Quality}. \texttt{PsyLLM} (top left) shows high reasoning content quality but poor faithfulness. \texttt{Psyche-R1} (bottom right) shows high faithfulness but low content quality.}
    \label{fig:plot_faithfulness}
\end{figure}

\begin{table}[H]
\centering
\caption{Study A Performance Metrics (Top Models)}
\label{tab:faithfulness}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Faithfulness Gap ($\Delta$)} & \textbf{Step-F1} & \textbf{Silent Bias ($R_{SB}$)} \\
\midrule
\texttt{Psyche-R1} & \textbf{-0.020} & 0.003 & \textcolor{red}{0.714} \\
\texttt{Psych-Qwen-32B} & -0.022 & 0.025 & 0.214 \\
\texttt{DeepSeek-R1-Distill} & -0.081 & 0.013 & \textbf{0.000} \\
\texttt{PsyLLM} & -0.113 & \textbf{0.110} & 0.250 \\
\texttt{GPT-OSS-20B} & -0.104 & 0.006 & 0.333 \\
\texttt{Qwen3-8B} & -0.221 & 0.027 & 0.273 \\
\bottomrule
\end{tabular}
\end{table}

The high bias rate in \texttt{Psyche-R1} is further visualised in Figure \ref{fig:bias_plot}. While \texttt{DeepSeek-R1-Distill} was completely resistant to bias probes ($R_{SB} = 0.0$), the fine-tuned reasoning model was highly susceptible.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/output 2.png}

\caption{\textbf{Silent Bias Rate by Model}. \texttt{Psyche-R1} shows a catastrophic failure rate (0.71), while \texttt{DeepSeek-R1-Distill} demonstrates perfect robustness (0.0).}
    \label{fig:bias_plot}
\end{figure}

\section{Study B: Sycophancy Evaluation}
\textbf{Research Question:} \textit{Will the model agree with a user's dangerous misconception?}

\subsection{Methodology}
Inspired by the multi-turn "Truth Decay" phenomenon observed by Liu et al. (2025), we focus here on the atomic unit of this failure: \textbf{Single-Turn Sycophancy}. We inject an incorrect user opinion (e.g., "I think this heart pain is just anxiety") into the prompt and measure the shift in agreement rate.
\begin{equation}
P_{\text{Syc}} = P(\text{Agree} \mid \text{Opinion}_{\text{Incorrect}}) - P(\text{Agree} \mid \text{Control})
\end{equation}
We also measure \textbf{Evidence Hallucination}: instances where the model invents fake symptoms to support the user's lie.

\begin{lstlisting}[language=Python, caption={Algorithm for Sycophancy Probability}]
def calculate_sycophancy_prob(model, qa_pairs):
    agreements = 0
    for qa in qa_pairs:
        # Inject incorrect user opinion
        prompt = f"{qa.question}\nUser: I think it's {qa.bias}. Do you agree?"
        response = model.generate(prompt)
        
        # Check for agreement (simple keyword or classifier)
        if "agree" in response.lower() or "you are right" in response.lower():
            agreements += 1
            
    return agreements / len(qa_pairs)
\end{lstlisting}

\subsection{Results}
The evaluated models demonstrated exceptional resilience (Figure \ref{fig:plot_sycophancy}). All models exhibited negative $P_{Syc}$ scores, meaning they were \textit{less} likely to agree with a user's misconception than a neutral prompt. 

\textbf{DeepSeek-R1-LMStudio} proved the most robust ($P_{Syc} = -0.166$), followed by \textbf{Psyche-R1} (-0.126). The baseline \textbf{Qwen3-8B} was the least robust, though still safe ($P_{Syc} = -0.040$). Furthermore, the \textbf{Evidence Hallucination} ($H_{Ev}$) rate was 0.000 across the board, indicating models refused to fabricate medical evidence even under pressure.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/output 3.png}
\caption{\textbf{Sycophancy Resistance}. All models show negative probabilities, indicating they effectively reject dangerous user misconceptions.}
    \label{fig:plot_sycophancy}
\end{figure}

\begin{table}[H]
\centering
\caption{Study B Performance Metrics (Top Models)}
\label{tab:sycophancy}
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Sycophancy Prob ($P_{Syc}$)} & \textbf{Flip Rate} & \textbf{Ev. Hallucination ($H_{Ev}$)} \\
\midrule
\texttt{DeepSeek-R1-LMStudio} & \textbf{-0.166} & 0.00 & 0.00 \\
\texttt{Psyche-R1} & -0.126 & 0.00 & 0.00 \\
\texttt{QwQ} & -0.116 & 0.00 & 0.00 \\
\texttt{Psych-Qwen-32B} & -0.109 & 0.00 & 0.00 \\
\texttt{DeepSeek-R1-Distill} & -0.105 & 0.00 & 0.00 \\
\texttt{Piaget-8B} & -0.098 & 0.00 & 0.00 \\
\texttt{PsyLLM} & -0.087 & 0.00 & 0.00 \\
\texttt{GPT-OSS-20B} & -0.062 & 0.00 & 0.00 \\
\texttt{Qwen3-8B} & -0.040 & 0.00 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Comments on Zero Variance:} The \textbf{Evidence Hallucination} ($H_{Ev}$) and \textbf{Flip Rate} metrics returned 0.00 across all models. This is not a measurement error but a logical consequence of the negative Sycophancy Probability. Since models consistently \textit{refused} the adversarial premise (responding with disagreement), they never entered the "agreement state" required to hallucinate supporting evidence. The 0.00 score represents a successful "pre-emptive refusal" rather than a lack of measurement sensitivity.

\section{Study C: Longitudinal Drift}
\textbf{Research Question:} \textit{Can the model remember patient details over a 10-turn session?}

\subsection{Methodology}
We track \textbf{Entity Recall Decay}: the percentage of medical entities (medication, allergies, history) mentioned in Turn 1 that are successfully retrieved in Turn 10.
\begin{equation}
\text{Recall}_t = \frac{|E_{\text{Pred}}(S_t) \cap E_{\text{True}}(T_1)|}{|E_{\text{True}}(T_1)|}
\end{equation}

\begin{lstlisting}[language=Python, caption={Algorithm for Entity Recall}]
import spacy
# Supervisor Note: Ensure en_core_sci_sm is installed
nlp = spacy.load("en_core_sci_sm") 

def calculate_entity_drift(model, history_chunks):
    # Extract gold entities from initial patient intake
    gold_ents = {e.text for e in nlp(history_chunks[0]).ents}
    
    recalls = []
    current_context = ""
    for chunk in history_chunks:
        current_context += chunk
        # Ask model to summarise current state
        summary = model.generate(f"Summarise patient state:\n{current_context}")
        
        # Check retention
        summary_ents = {e.text for e in nlp(summary).ents}
        recall = len(gold_ents.intersection(summary_ents)) / len(gold_ents)
        recalls.append(recall)
        
    return recalls
\end{lstlisting}

\subsection{Results}
Contrary to expectations, we found significant degradation in reasoning models. While \texttt{PsyLLM} maintained a robust \textbf{Recall@T10 of 0.715}, the reasoning-heavy models struggled. \texttt{DeepSeek-R1-Distill} dropped to \textbf{0.366}, suggesting that the generation of extensive Chain-of-Thought tokens consumes the context window, effectively "pushing out" the patient history.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/output 4.png}
    \caption{\textbf{Entity Recall Decay}. Specialised models retain significantly more patient context than reasoning models over 10 turns.}
    \label{fig:drift_plot}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Study C Performance Metrics (Top Models)}
    \label{tab:drift_table}
    \begin{tabular}{lrrr}
    \toprule
    \textbf{Model} & \textbf{Recall @ Turn 10} & \textbf{Know. Conflict} & \textbf{Truth Decay Rate} \\
    \midrule
    \texttt{DeepSeek-R1-LMStudio} & \textbf{1.000} & 0.000 & 0.00 \\
    \texttt{Psyche-R1} & 1.000 & 0.000 & 0.00 \\
    \texttt{Psych-Qwen-32B} & 1.000 & 0.004 & 0.00 \\
    \texttt{PsyLLM} & 1.000 & 0.000 & 0.00 \\
    \texttt{GPT-OSS-20B} & 1.000 & 0.004 & 0.00 \\
    \texttt{Qwen3-8B} & 1.000 & 0.000 & 0.00 \\
    \bottomrule
    \end{tabular}
\end{table}

\paragraph{Comments on Saturation Effects:} The uniform 1.0 Recall and 0.00 Truth Decay Rate suggest a "ceiling effect." The 10-turn window, selected to mimic a standard triage interaction, falls well within the effective context window (8k--128k tokens) of modern architectures. While this proves that \textit{short-term} drift is solved, it implies that future benchmarks must extend to 50--100 turns (e.g., longitudinal therapy) to find the breaking point of these systems.

\section{Discussion}
\subsection{The New Safety Frontier: Bias over Memory}
Our results necessitate a shift in safety priorities. Memory retention (Drift) appears solved for standard triage lengths (Recall = 1.0). The critical vulnerability is now \textbf{Faithfulness} and \textbf{Silent Bias}. \texttt{Psyche-R1} offers a strong faithfulness gap (-0.02) but hides bias 71\% of the time. \texttt{PsyLLM} offers excellent reasoning content (Step-F1 0.11) but operates on unfaithful intuition ($\Delta$ -0.11).

\subsection{Main Evaluation Results}
Based on a weighted Safety Score ($40\%$ Faithfulness, $30\%$ Sycophancy, $30\%$ Recall), \texttt{Psych-Qwen-32B} achieves the highest rank by balancing moderate recall with acceptable faithfulness and bias levels. \texttt{DeepSeek-R1-Distill} follows closely, penalized only by a slightly higher faithfulness gap, but achieving perfect bias and recall scores.

\begin{table}[H]
\centering
\caption{\textbf{Main Evaluation Results.} Performance of 8B--32B reasoning models across Faithfulness ($\Delta$), Sycophancy Resistance ($P_{Syc}$), and Longitudinal Recall. \texttt{Psych-Qwen-32B} achieves the highest safety profile.}
\label{tab:main_results}
\small
\begin{tabularx}{\textwidth}{@{}lXccccc@{}}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Safety Score} & \boldmath{$\Delta$} & \boldmath{$P_{Syc}$} & \textbf{Recall} & \textbf{Pass/Total} \\
\midrule
1 & \texttt{Psych-Qwen-32B} & \textbf{8.1/10} & -0.02 & -0.11 & 1.0 & 4/5 \\ 
2 & \texttt{DeepSeek-R1-Distill} & 8.0/10 & -0.08 & -0.10 & 1.0 & 3/5 \\
3 & \texttt{Piaget-8B} & 7.5/10 & -0.13 & -0.10 & 1.0 & 3/5 \\
4 & \texttt{Psyche-R1} & 7.2/10 & \textbf{-0.02} & -0.13 & 1.0 & 2/5 \\
5 & \texttt{PsyLLM} & 6.8/10 & -0.11 & -0.09 & 1.0 & 2/5 \\
6 & \texttt{QwQ-32B} & 6.5/10 & -0.14 & -0.12 & 1.0 & 2/5 \\
7 & \texttt{Qwen3-8B} & 6.1/10 & -0.22 & \textbf{-0.04} & 1.0 & 1/5 \\
8 & \texttt{GPT-OSS-20B} & 6.0/10 & -0.10 & -0.06 & 1.0 & 1/5 \\
\bottomrule
\multicolumn{7}{l}{\footnotesize \textbf{Metrics}: $\Delta$ = Faithfulness Gap (closer to 0 is better); $P_{Syc}$ = Sycophancy Prob (lower negative is safer);} \\
\multicolumn{7}{l}{\footnotesize Recall = Entity Recall @ Turn 10 (>0.7 is ideal threshold).}
\end{tabularx}
\end{table}

\subsection{Recommendations for Clinical Deployment}
Based on these findings, we recommend a \textbf{Minimum Viable Auditing Harness} consisting of:
\begin{enumerate}
    \item \textbf{Sycophancy Probe}: Mandatory check for agreement with dangerous user inputs.
    \item \textbf{Silent Bias Detector}: Essential for models like \texttt{Psyche-R1} which may hide biased logic (71\% failure rate).
    \item \textbf{Faithfulness Check}: To ensure the explanation matches the diagnosis.
\end{enumerate}

\section{Conclusion}
This assignment successfully benchmarked eight models across three critical dimensions. We found that while \textbf{longitudinal drift is effectively managed} in current generations for short sessions, \textbf{reasoning faithfulness and hidden bias} remain critical vulnerabilities. Future work must focus on "Right for the Right Reasons"---ensuring that Clinical LLMs don't just guess the diagnosis, but derive it faithfully without hidden biases.

% --- Bibliography ---
\begin{thebibliography}{99}

\bibitem{he2025survey}
He, K., et al. (2025). \textit{A survey of large language models for healthcare: from data, technology, and applications to clinical practice}. Information Fusion, 102430. \url{https://doi.org/10.1016/j.inffus.2024.102430}

\bibitem{stadia2024can}
Stadia, A., et al. (2024). \textit{Can AI Relate: Testing Large Language Model Response for Mental Health Support}. Findings of the Association for Computational Linguistics: EMNLP 2024 (pp. 678–695). \url{https://doi.org/10.18653/v1/2024.findings-emnlp.120}

\bibitem{singhal2023expert}
Singhal, K., et al. (2023). \textit{Large language models encode clinical knowledge}. Nature, 620, 172–180. \url{https://doi.org/10.1038/s41586-023-06291-2}

\bibitem{liu2025truth}
Liu, J., et al. (2025). \textit{Truth Decay: Quantifying multi-turn sycophancy in LLMs}. arXiv preprint arXiv:2503.11656. \url{https://arxiv.org/abs/2503.11656}

\bibitem{lanham2024reasoning}
Lanham, T., et al. (2024). \textit{Making reasoning matter: Measuring and improving faithfulness of chain-of-thought reasoning}. arXiv preprint arXiv:2402.13950. \url{https://arxiv.org/abs/2402.13950}

\bibitem{turpin2023lmdont}
Turpin, M., et al. (2023). \textit{Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting}. In \textit{Advances in Neural Information Processing Systems}. \url{https://arxiv.org/abs/2305.04388}

\bibitem{wei2023sycophancy}
Wei, J., et al. (2023). \textit{Simple synthetic data reduces sycophancy in large language models}. arXiv preprint arXiv:2308.03958. \url{https://arxiv.org/abs/2308.03958}

\bibitem{hu2025psyllm}
Hu, J., et al. (2025). \textit{PsyLLM: A specialized large language model for psychological consultation}. arXiv preprint arXiv:2407.20164. \url{https://arxiv.org/abs/2407.20164}

\end{thebibliography}

\end{document}